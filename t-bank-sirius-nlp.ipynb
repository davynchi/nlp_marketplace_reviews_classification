{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что происходит в этом ноутбуке\n",
    "Для начала: задача в этом ноутбуке -- разметить негативные отзывы с алиэкспресса.\n",
    "На что именно покупатели с алика чаще всего жалуются. Видимо, это для команды приложения\n",
    "\"Шопинг\" Т-банка.\n",
    "1. Сначала я разметил данные через LLM-ку. В качестве модели я выбрал квантизованную\n",
    "версию T-lite-it-1.0, так как это русскоязычная модель и так как при обучении могли использоваться данные\n",
    "с приложения \"Шопинг\" Т-банка, поэтому она будет \"легче\" понимать тексты.\n",
    "2. Затем я использовал snorkel для дополнительной разметки (это регулярки, решил, что без них здесь никак).\n",
    "3. Предсказанные классы (и вероятности) от LLM я скомбинировал по формуле $0.6\\cdot llm + 0.4\\cdot snorkel$,\n",
    "все, что ниже порога, отнес к \"нет товара\" (модель не уверена в товарах).\n",
    "4. Далее нагенерировал синтетические примеры через ту же модель.\n",
    "5. Затем я аугментировал примеры через замену слов синонимами, замену местами соседних слов и\n",
    "замену случайных букв.\n",
    "6. Наконец, я использовал QLoRA на той же модели для дообучения на примерах. Я добавил 8 токенов,\n",
    "соответствующих каждой категории товаров, и дообучал модель предсказывать 1 токен товара.\n",
    "На валидационной выборке удалось достичь качества 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-17T04:19:15.140686Z",
     "iopub.status.busy": "2025-09-17T04:19:15.140469Z",
     "iopub.status.idle": "2025-09-17T04:19:16.244775Z",
     "shell.execute_reply": "2025-09-17T04:19:16.243984Z",
     "shell.execute_reply.started": "2025-09-17T04:19:15.140669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/t-bank-nlp/NLP_case/submission_example.csv\n",
      "/kaggle/input/t-bank-nlp/NLP_case/categories.txt\n",
      "/kaggle/input/t-bank-nlp/NLP_case/README.md\n",
      "/kaggle/input/t-bank-nlp/NLP_case/train.csv\n",
      "/kaggle/input/t-bank-nlp/NLP_case/test.csv\n",
      "/kaggle/input/train-predictions/train_augmented.parquet\n",
      "/kaggle/input/train-predictions/synthetic_data.parquet\n",
      "/kaggle/input/train-predictions/llm_predictions.parquet\n",
      "/kaggle/input/train-predictions/train_with_labels.parquet\n",
      "/kaggle/input/train-predictions/snorkel_predictions.parquet\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:19:57.492552Z",
     "iopub.status.busy": "2025-09-17T04:19:57.491884Z",
     "iopub.status.idle": "2025-09-17T04:19:57.496511Z",
     "shell.execute_reply": "2025-09-17T04:19:57.495827Z",
     "shell.execute_reply.started": "2025-09-17T04:19:57.492501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:19:58.446213Z",
     "iopub.status.busy": "2025-09-15T08:19:58.445997Z",
     "iopub.status.idle": "2025-09-15T08:19:58.467672Z",
     "shell.execute_reply": "2025-09-15T08:19:58.466789Z",
     "shell.execute_reply.started": "2025-09-15T08:19:58.446194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('/kaggle/input/t-bank-nlp/NLP_case/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:19:58.470126Z",
     "iopub.status.busy": "2025-09-15T08:19:58.469295Z",
     "iopub.status.idle": "2025-09-15T08:19:58.480735Z",
     "shell.execute_reply": "2025-09-15T08:19:58.480064Z",
     "shell.execute_reply.started": "2025-09-15T08:19:58.470099Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['обувь', 'одежда', 'посуда', 'текстиль', 'товары для детей',\n",
       "       'украшения и аксессуары', 'электроника', 'нет товара'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = pd.read_csv('/kaggle/input/t-bank-nlp/NLP_case/categories.txt')\n",
    "categories.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T08:12:17.989568Z",
     "iopub.status.busy": "2025-09-17T08:12:17.989268Z",
     "iopub.status.idle": "2025-09-17T08:12:18.054355Z",
     "shell.execute_reply": "2025-09-17T08:12:18.053566Z",
     "shell.execute_reply.started": "2025-09-17T08:12:17.989545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/kaggle/input/t-bank-nlp/NLP_case/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:19:58.569994Z",
     "iopub.status.busy": "2025-09-15T08:19:58.569730Z",
     "iopub.status.idle": "2025-09-15T08:19:58.577047Z",
     "shell.execute_reply": "2025-09-15T08:19:58.576245Z",
     "shell.execute_reply.started": "2025-09-15T08:19:58.569969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CLASSES = ['обувь','одежда','посуда','текстиль','товары для детей',\n",
    "           'украшения и аксессуары','электроника','нет товара']\n",
    "IDX = {c:i for i,c in enumerate(CLASSES)}\n",
    "ABSTAIN = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:19:58.578114Z",
     "iopub.status.busy": "2025-09-15T08:19:58.577807Z",
     "iopub.status.idle": "2025-09-15T08:19:58.599149Z",
     "shell.execute_reply": "2025-09-15T08:19:58.598238Z",
     "shell.execute_reply.started": "2025-09-15T08:19:58.578082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>Маленькие. Когда одеваешь рисунка почти нет. Б...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "853  Маленькие. Когда одеваешь рисунка почти нет. Б..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['text'].str.contains(r'подошв')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:19:58.600355Z",
     "iopub.status.busy": "2025-09-15T08:19:58.600057Z",
     "iopub.status.idle": "2025-09-15T08:19:58.616536Z",
     "shell.execute_reply": "2025-09-15T08:19:58.615965Z",
     "shell.execute_reply.started": "2025-09-15T08:19:58.600328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(lambda x: x.strip().lower().replace('\\r', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:19:58.617564Z",
     "iopub.status.busy": "2025-09-15T08:19:58.617343Z",
     "iopub.status.idle": "2025-09-15T08:19:58.637998Z",
     "shell.execute_reply": "2025-09-15T08:19:58.637312Z",
     "shell.execute_reply.started": "2025-09-15T08:19:58.617535Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>заказали 14.10.2017 , получили 25.10.2017 \\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>футболка хорошего качества,но футболка не как ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>все отлично!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>рисунок не очень чёткий, а ткань прозрачная, в...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>плохо!!!низ рваный..деньги не вернули!открыла ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>спасибо,подошло по размеру.все понравилось в п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>доставка быстрая, до саратова около 2 недель. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>на внешний вид шапка нормальная, на большой об...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>за 4 месяца товар так и не дошел до покупателя.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>прислал пальто вместо куртки!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1818 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     заказали 14.10.2017 , получили 25.10.2017 \\n\\n...\n",
       "1     футболка хорошего качества,но футболка не как ...\n",
       "2                                        все отлично!!!\n",
       "3     рисунок не очень чёткий, а ткань прозрачная, в...\n",
       "4     плохо!!!низ рваный..деньги не вернули!открыла ...\n",
       "...                                                 ...\n",
       "1813  спасибо,подошло по размеру.все понравилось в п...\n",
       "1814  доставка быстрая, до саратова около 2 недель. ...\n",
       "1815  на внешний вид шапка нормальная, на большой об...\n",
       "1816    за 4 месяца товар так и не дошел до покупателя.\n",
       "1817                    прислал пальто вместо куртки!!!\n",
       "\n",
       "[1818 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T08:12:34.141606Z",
     "iopub.status.busy": "2025-09-17T08:12:34.141293Z",
     "iopub.status.idle": "2025-09-17T08:12:34.156539Z",
     "shell.execute_reply": "2025-09-17T08:12:34.155978Z",
     "shell.execute_reply.started": "2025-09-17T08:12:34.141584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test['text'] = df_test['text'].apply(lambda x: x.strip().lower().replace('\\r', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T08:12:44.227652Z",
     "iopub.status.busy": "2025-09-17T08:12:44.226821Z",
     "iopub.status.idle": "2025-09-17T08:12:44.232888Z",
     "shell.execute_reply": "2025-09-17T08:12:44.232165Z",
     "shell.execute_reply.started": "2025-09-17T08:12:44.227621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df_train['text'] = df_train['text'].fillna(\"\")\n",
    "df_test['text'] = df_test['text'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T08:17:12.731231Z",
     "iopub.status.busy": "2025-09-17T08:17:12.730671Z",
     "iopub.status.idle": "2025-09-17T08:17:12.739790Z",
     "shell.execute_reply": "2025-09-17T08:17:12.739075Z",
     "shell.execute_reply.started": "2025-09-17T08:17:12.731210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text]\n",
       "Index: []"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['text'] == '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузим модель, составим промт, дадим вероятности категории"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM для разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-14T11:10:59.528881Z",
     "iopub.status.busy": "2025-09-14T11:10:59.528225Z",
     "iopub.status.idle": "2025-09-14T11:12:27.037115Z",
     "shell.execute_reply": "2025-09-14T11:12:27.036425Z",
     "shell.execute_reply.started": "2025-09-14T11:10:59.528860Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, accelerate\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.1\n",
      "    Uninstalling huggingface-hub-0.33.1:\n",
      "      Successfully uninstalled huggingface-hub-0.33.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.8.1\n",
      "    Uninstalling accelerate-1.8.1:\n",
      "      Successfully uninstalled accelerate-1.8.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.10.1 huggingface-hub-0.34.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tokenizers-0.22.0 transformers-4.56.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-14T11:12:27.038377Z",
     "iopub.status.busy": "2025-09-14T11:12:27.038131Z",
     "iopub.status.idle": "2025-09-14T11:12:35.613015Z",
     "shell.execute_reply": "2025-09-14T11:12:35.612270Z",
     "shell.execute_reply.started": "2025-09-14T11:12:27.038350Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.47.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:12:35.614255Z",
     "iopub.status.busy": "2025-09-14T11:12:35.613984Z",
     "iopub.status.idle": "2025-09-14T11:12:38.871607Z",
     "shell.execute_reply": "2025-09-14T11:12:38.870941Z",
     "shell.execute_reply.started": "2025-09-14T11:12:35.614216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.6.0+cu124 CUDA: 12.4\n",
      "GPU visible: None\n",
      "Linux-6.6.56+-x86_64-with-glibc2.35\n"
     ]
    }
   ],
   "source": [
    "import torch, os, platform\n",
    "print(\"torch:\", torch.__version__, \"CUDA:\", torch.version.cuda)\n",
    "print(\"GPU visible:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(platform.platform())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:12:38.872761Z",
     "iopub.status.busy": "2025-09-14T11:12:38.872292Z",
     "iopub.status.idle": "2025-09-14T11:12:39.043379Z",
     "shell.execute_reply": "2025-09-14T11:12:39.042524Z",
     "shell.execute_reply.started": "2025-09-14T11:12:38.872728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:12:39.044732Z",
     "iopub.status.busy": "2025-09-14T11:12:39.044339Z",
     "iopub.status.idle": "2025-09-14T11:12:40.175060Z",
     "shell.execute_reply": "2025-09-14T11:12:40.174294Z",
     "shell.execute_reply.started": "2025-09-14T11:12:39.044706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "DTYPE = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=DTYPE, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:14:54.433340Z",
     "iopub.status.busy": "2025-09-14T11:14:54.433054Z",
     "iopub.status.idle": "2025-09-14T11:18:10.149105Z",
     "shell.execute_reply": "2025-09-14T11:18:10.148518Z",
     "shell.execute_reply.started": "2025-09-14T11:14:54.433320Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e9055774dc47b2917204544d4ca013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7ef18a10f14823b256ee4bafcd56a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442c78a4cdf84d338e79fdaeb626f605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ed6fa55b82431a8c878a03a759f7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4453fb9ad8b4d8aa0c5d2b971e689d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/712 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 11:14:59.030041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757848499.236878      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757848499.291970      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd200f7dd736474a8078cffbf7fc0d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52226f96ff334a3e90e8f63f32b18f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc02b9f821947199a6f30e6aac6716a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66dfb2500dde4024abadf52860253499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa057aa27604b72b058c30b4bfa98ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46e8ec7d329443481e89e2337c9ae64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fe809c00bd493fa9d1a99c6d1da9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b053e42ed34c2eb20d653de08da362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_name = \"t-tech/T-lite-it-1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_state_dict=True,\n",
    "    quantization_config=bnb_cfg,\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:18:10.150866Z",
     "iopub.status.busy": "2025-09-14T11:18:10.150309Z",
     "iopub.status.idle": "2025-09-14T11:18:10.155471Z",
     "shell.execute_reply": "2025-09-14T11:18:10.154689Z",
     "shell.execute_reply.started": "2025-09-14T11:18:10.150847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classificator_prompt = '''\n",
    "System: Ты — аккуратный классификатор отзывов маркетплейса по категориям товаров, к которым они относятся.\n",
    "Доступные метки:\n",
    "обувь,\n",
    "одежда,\n",
    "посуда,\n",
    "текстиль,\n",
    "товары для детей,\n",
    "украшения и аксессуары,\n",
    "электроника,\n",
    "нет товара.\n",
    "\n",
    "Правила:\n",
    "- Если отзыв только про доставку/продавца/деньги/спор без товара → \"нет товара\".\n",
    "- Если не ясно, какой товар — тоже \"нет товара\".\n",
    "- Выбери ровно одну метку.\n",
    "\n",
    "Ответь строго JSON одной строкой:\n",
    "{\"label\": \"<одна метка>\", \"confidence\": 0..1, \"reason\": \"<до 12 слов>\"}\n",
    "\n",
    "Примеры:\n",
    "\n",
    "Отзыв: отличная блузка отслеживалась пришла за 10 дней.\n",
    "Ответ: {\"label\": \"одежда\", \"confidence\": 1, \"reason\": \"блузка относится к одежде\"}\n",
    "\n",
    "Отзыв: Лучшая покупка на алиэкспресс !!)))\n",
    "Ответ: {\"label\": \"нет товара\", \"confidence\": 1, \"reason\": \"нет никакого описания товара\"}\n",
    "\n",
    "Отзыв: На фото они матовые, а пришли глянцевые.\n",
    "Ответ: {\"label\": \"обувь\", \"confidence\": 0.4, \"reason\": \"матовость и глянцевость в множественном числе, вероятно, относится к обуви\"}\n",
    "\n",
    "Отзыв: Товар пришёл менее чем за месяц. На мой 44 заказала L пришла необъятных размеров  \n",
    "Ответ: {\"label\": \"одежда\", \"confidence\": 0.9, \"reason\": \"Размер L почти всегда относится к одежде\"}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:18:10.156593Z",
     "iopub.status.busy": "2025-09-14T11:18:10.156293Z",
     "iopub.status.idle": "2025-09-14T11:18:10.173168Z",
     "shell.execute_reply": "2025-09-14T11:18:10.172433Z",
     "shell.execute_reply.started": "2025-09-14T11:18:10.156567Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151665, 3584, padding_idx=151643)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=151665, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:18:10.175048Z",
     "iopub.status.busy": "2025-09-14T11:18:10.174688Z",
     "iopub.status.idle": "2025-09-14T11:18:10.184995Z",
     "shell.execute_reply": "2025-09-14T11:18:10.184434Z",
     "shell.execute_reply.started": "2025-09-14T11:18:10.175031Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'товар не пришел. деньги вернули. но продавец хороший и милый. просто, может быть, мне не повезло.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'].iloc[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:19:29.738267Z",
     "iopub.status.busy": "2025-09-14T11:19:29.737977Z",
     "iopub.status.idle": "2025-09-14T11:19:33.117135Z",
     "shell.execute_reply": "2025-09-14T11:19:33.116341Z",
     "shell.execute_reply.started": "2025-09-14T11:19:29.738247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"label\": \"нет товара\", \"confidence\": 1, \"reason\": \"отзыв только о доставке и продавце\"}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": classificator_prompt},\n",
    "    {\"role\": \"user\", \"content\": df_train['text'].iloc[110]}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=256\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:19:35.845781Z",
     "iopub.status.busy": "2025-09-14T11:19:35.845063Z",
     "iopub.status.idle": "2025-09-14T11:19:35.852569Z",
     "shell.execute_reply": "2025-09-14T11:19:35.851853Z",
     "shell.execute_reply.started": "2025-09-14T11:19:35.845753Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.051357746124268"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters()) / 1024 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T10:28:45.454421Z",
     "iopub.status.busy": "2025-09-14T10:28:45.453807Z",
     "iopub.status.idle": "2025-09-14T10:28:45.458878Z",
     "shell.execute_reply": "2025-09-14T10:28:45.458277Z",
     "shell.execute_reply.started": "2025-09-14T10:28:45.454398Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:21:39.182014Z",
     "iopub.status.busy": "2025-09-14T11:21:39.181701Z",
     "iopub.status.idle": "2025-09-14T12:28:50.727569Z",
     "shell.execute_reply": "2025-09-14T12:28:50.726838Z",
     "shell.execute_reply.started": "2025-09-14T11:21:39.181992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e0c16572db409f9d21dc2b2f0e6574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM labeling:   0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_from_llm\n",
      "одежда                    1133\n",
      "нет товара                 568\n",
      "обувь                       55\n",
      "текстиль                    28\n",
      "украшения и аксессуары      19\n",
      "посуда                      11\n",
      "товары для детей             4\n",
      "Name: count, dtype: int64\n",
      "Mean confidence: 0.847\n"
     ]
    }
   ],
   "source": [
    "# Fast batched LLM labeling → adds \"class_from_llm\" and \"llm_confidence\" to df_train\n",
    "\n",
    "import re, json, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # more efficient for decoder-only generation\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# --- Helpers: prompt builder, robust JSON parser ---\n",
    "def make_prompt(txt: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": classificator_prompt},   # <- you already defined this\n",
    "        {\"role\": \"user\", \"content\": txt},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def extract_json(s: str):\n",
    "    # Try a quick slice between first '{' and last '}' (common for chatty models)\n",
    "    try:\n",
    "        i, j = s.find(\"{\"), s.rfind(\"}\")\n",
    "        if i != -1 and j != -1 and j > i:\n",
    "            return json.loads(s[i:j+1])\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback regex\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def parse_output(text: str):\n",
    "    obj = extract_json(text) or {}\n",
    "    label = obj.get(\"label\", None)\n",
    "    conf  = obj.get(\"confidence\", None)\n",
    "    # normalize label\n",
    "    if isinstance(label, str):\n",
    "        label = label.strip().lower()\n",
    "    if label not in CLASSES:\n",
    "        # soft match (e.g., extra words)\n",
    "        for c in CLASSES:\n",
    "            if label and c in label:\n",
    "                label = c; break\n",
    "    if label not in CLASSES:\n",
    "        label = \"нет товара\"   # safe fallback\n",
    "    # coerce confidence\n",
    "    try:\n",
    "        conf = float(conf)\n",
    "        if not (0.0 <= conf <= 1.0):\n",
    "            conf = 0.5\n",
    "    except Exception:\n",
    "        conf = 0.5\n",
    "    return label, conf\n",
    "\n",
    "# --- Prepare data ---\n",
    "df_train[\"text\"] = df_train[\"text\"].fillna(\"\").astype(str)\n",
    "texts = df_train[\"text\"].tolist()\n",
    "\n",
    "# --- Fast batched generation settings ---\n",
    "BATCH_SIZE = 8            # ↓ if OOM, ↑ if room\n",
    "MAX_INPUT_TOKENS = 512     # cap context for speed\n",
    "MAX_NEW_TOKENS = 256        # JSON answer is short\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    do_sample=False,            # deterministic & fast\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "pred_labels, pred_conf = [], []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for start in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"LLM labeling\"):\n",
    "        batch_texts = texts[start:start+BATCH_SIZE]\n",
    "        prompts = [make_prompt(t) for t in batch_texts]\n",
    "\n",
    "        enc = tokenizer(\n",
    "            prompts, return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=MAX_INPUT_TOKENS\n",
    "        ).to(model.device)\n",
    "\n",
    "        input_lengths = enc[\"attention_mask\"].sum(dim=1)  # prompt lengths per sample\n",
    "\n",
    "        gen = model.generate(**enc, **GEN_KW)\n",
    "\n",
    "        # Slice out only the newly generated tokens for each row\n",
    "        new_tokens = [out_ids[in_len:] for out_ids, in_len in zip(gen, input_lengths)]\n",
    "\n",
    "        # Pad to a rectangular tensor just for vectorized decode, then decode\n",
    "        padded = pad_sequence(new_tokens, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        out_texts = tokenizer.batch_decode(padded, skip_special_tokens=True)\n",
    "\n",
    "        for out in out_texts:\n",
    "            lbl, conf = parse_output(out)\n",
    "            pred_labels.append(lbl)\n",
    "            pred_conf.append(conf)\n",
    "\n",
    "# --- Attach results to the DataFrame ---\n",
    "df_train[\"class_from_llm\"]   = pred_labels\n",
    "df_train[\"llm_confidence\"]   = pred_conf\n",
    "\n",
    "# Optional quick sanity check\n",
    "print(df_train[\"class_from_llm\"].value_counts(dropna=False))\n",
    "print(f\"Mean confidence: {np.mean(pred_conf):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T12:33:50.323500Z",
     "iopub.status.busy": "2025-09-14T12:33:50.323198Z",
     "iopub.status.idle": "2025-09-14T12:33:50.494844Z",
     "shell.execute_reply": "2025-09-14T12:33:50.494251Z",
     "shell.execute_reply.started": "2025-09-14T12:33:50.323479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.to_parquet('train_predictions.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snorkel для разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-15T08:03:25.333442Z",
     "iopub.status.busy": "2025-09-15T08:03:25.332966Z",
     "iopub.status.idle": "2025-09-15T08:04:36.403684Z",
     "shell.execute_reply": "2025-09-15T08:04:36.402722Z",
     "shell.execute_reply.started": "2025-09-15T08:03:25.333420Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snorkel\n",
      "  Downloading snorkel-0.10.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting munkres>=1.0.6 (from snorkel)\n",
      "  Downloading munkres-1.1.4-py2.py3-none-any.whl.metadata (980 bytes)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from snorkel) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from snorkel) (1.15.3)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from snorkel) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.11/dist-packages (from snorkel) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.2 in /usr/local/lib/python3.11/dist-packages (from snorkel) (1.2.2)\n",
      "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from snorkel) (2.6.0+cu124)\n",
      "Requirement already satisfied: tensorboard>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from snorkel) (2.18.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from snorkel) (3.20.3)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.11/dist-packages (from snorkel) (3.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.0->snorkel) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.0->snorkel) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.0->snorkel) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.0->snorkel) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.0->snorkel) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.0->snorkel) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->snorkel) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->snorkel) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->snorkel) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.2->snorkel) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.2->snorkel) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->snorkel) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->snorkel) (1.73.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->snorkel) (3.8.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->snorkel) (25.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->snorkel) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->snorkel) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->snorkel) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->snorkel) (3.1.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->snorkel) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->snorkel) (4.14.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->snorkel) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->snorkel) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.2.0->snorkel)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.2.0->snorkel)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.2.0->snorkel)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.2.0->snorkel)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.2.0->snorkel)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.2.0->snorkel)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.2.0->snorkel)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.2.0->snorkel)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.2.0->snorkel)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->snorkel) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->snorkel) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->snorkel) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.2.0->snorkel)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->snorkel) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->snorkel) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.2.0->snorkel) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.13.0->snorkel) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.0->snorkel) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.0->snorkel) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.0->snorkel) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24.0->snorkel) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24.0->snorkel) (2024.2.0)\n",
      "Downloading snorkel-0.10.0-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: munkres, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, snorkel\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed munkres-1.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 snorkel-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:04:36.405674Z",
     "iopub.status.busy": "2025-09-15T08:04:36.405415Z",
     "iopub.status.idle": "2025-09-15T08:04:40.333789Z",
     "shell.execute_reply": "2025-09-15T08:04:40.333207Z",
     "shell.execute_reply.started": "2025-09-15T08:04:36.405650Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re, numpy as np\n",
    "from snorkel.labeling import labeling_function           # decorators still here\n",
    "from snorkel.labeling import PandasLFApplier       # appliers moved to .apply\n",
    "from snorkel.labeling.model import LabelModel            # LabelModel lives here\n",
    "from snorkel.labeling.analysis import LFAnalysis         # analysis lives here\n",
    "\n",
    "CLASSES = ['обувь','одежда','посуда','текстиль','товары для детей',\n",
    "           'украшения и аксессуары','электроника','нет товара']\n",
    "IDX = {c:i for i,c in enumerate(CLASSES)}\n",
    "ABSTAIN = -1\n",
    "\n",
    "# --- helpers\n",
    "def has(text, rx): return re.search(rx, text, flags=re.I) is not None\n",
    "\n",
    "# --- LFs (examples; extend!)\n",
    "@labeling_function()\n",
    "def lf_shoes_kw(x):\n",
    "    if has(x.text, r'\\b(ботинк|сапог|кроссовк|кеды|туфл|угг|босоножк|шл[её]панц|ботильон|валенк|)\\w*\\b'):\n",
    "        return IDX['обувь']\n",
    "    if has(x.text, r'\\b(стельк|подошв|каблук|колодк)\\w*\\b'):\n",
    "        return IDX['обувь']\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_clothes_kw(x):\n",
    "    if has(x.text, r'(пуховик|куртк|ветровк|футболк|плать|джинс|рубашк|юбк|свитер|кофт|худи|термобель|колготк|пижам|халат|шорт|шарф|шапк|варежк|носки| S | M | L | XL | s | m | l | xl | м | л | с)\\w*'):\n",
    "        return IDX['одежда']\n",
    "    if has(x.text, r'\\b(рост|обхват|талия|бедра)\\b') or has(x.text, r'(ткан|сшит)'):\n",
    "        return IDX['одежда']\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_textile_kw(x):\n",
    "    if has(x.text, r'(постельн|простын|наволочк|пододеяльник|полотенц|плед|покрывал|штор|тюл|скатерт|отрез ткан)\\w*'):\n",
    "        return IDX['текстиль']\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_tableware_kw(x):\n",
    "    if has(x.text, r'(кружк|тарелк|кастрюл|сковород|ковш|форм[аы] для запекан|бокал|графин|ложк|вилк)\\w*'):\n",
    "        return IDX['посуда']\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_jewelry_kw(x):\n",
    "    if has(x.text, r'(серьг|кольц|браслет|цепочк|кулон|чокер|брош|ремень|перчатк|очки|оправ)\\w*'):\n",
    "        return IDX['украшения и аксессуары']\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_electronics_kw(x):\n",
    "    if has(x.text, r'(телефон|смартфон|наушник|колонк|ноутбук|телевизор|планшет|экран|камер|bluetooth|заряд|кабель|блок питания|ламп(а|очку)|пылесос|фен|триммер|блендер|миксер|электрочайн)\\w*'):\n",
    "        return IDX['электроника']\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_kids_kw(x):\n",
    "    if has(x.text, r'(игрушк|кукл|конструктор|машинк|пазл|коляск|подгузник|бутылочк|соск|слюнявчик|погремушк|развива)\\w*'):\n",
    "        return IDX['товары для детей']\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_noproduct_service(x):\n",
    "    if has(x.text, r'(доставк|курьер|трек|спор|возврат|деньг|продавц|магазин|приложени|оплат|скидк|купон)\\w*'):\n",
    "        return IDX['нет товара']\n",
    "    if lf_shoes_kw(x) == ABSTAIN and lf_clothes_kw(x) == ABSTAIN and lf_textile_kw(x) == ABSTAIN \\\n",
    "      and lf_tableware_kw(x) == ABSTAIN and lf_jewelry_kw(x) == ABSTAIN and lf_electronics_kw(x) == ABSTAIN \\\n",
    "      and lf_kids_kw(x) == ABSTAIN:\n",
    "        return IDX['нет товара']\n",
    "    return ABSTAIN\n",
    "\n",
    "# @labeling_function()\n",
    "# def lf_too_short_generic(x):\n",
    "#     if len(re.findall(r'\\w+', x.text)) < 4 or has(x.text, r'товар|вернули'):\n",
    "#         return IDX['нет товара']\n",
    "#     return ABSTAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:05:42.027538Z",
     "iopub.status.busy": "2025-09-15T08:05:42.026704Z",
     "iopub.status.idle": "2025-09-15T08:05:46.117706Z",
     "shell.execute_reply": "2025-09-15T08:05:46.116883Z",
     "shell.execute_reply.started": "2025-09-15T08:05:42.027506Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1818/1818 [00:00<00:00, 5152.09it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 629.10epoch/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "df_snorkel = pd.DataFrame({\n",
    "    \"text\": df_train[\"text\"].fillna(\"\").astype(str)\n",
    "})\n",
    "\n",
    "lfs = [lf_shoes_kw, lf_clothes_kw, lf_textile_kw, lf_tableware_kw,\n",
    "       lf_jewelry_kw, lf_electronics_kw, lf_kids_kw, lf_noproduct_service]\n",
    "applier = PandasLFApplier(lfs)\n",
    "L = applier.apply(df=df_snorkel)  # shape (N, num_LFs)\n",
    "\n",
    "LFAnalysis(L=L, lfs=lfs).lf_summary()\n",
    "\n",
    "label_model = LabelModel(cardinality=len(CLASSES), verbose=True)\n",
    "label_model.fit(L_train=L, n_epochs=500, log_freq=50, seed=42)\n",
    "\n",
    "probs = label_model.predict_proba(L)  # (N, 8)\n",
    "lf_pred = probs.argmax(1)\n",
    "lf_conf = probs.max(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:05:49.738869Z",
     "iopub.status.busy": "2025-09-15T08:05:49.738068Z",
     "iopub.status.idle": "2025-09-15T08:05:49.744190Z",
     "shell.execute_reply": "2025-09-15T08:05:49.743213Z",
     "shell.execute_reply.started": "2025-09-15T08:05:49.738842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17582843, 0.19520215, 0.09854587, ..., 0.09893634, 0.09854664,\n",
       "        0.13585023],\n",
       "       [0.17582843, 0.19520215, 0.09854587, ..., 0.09893634, 0.09854664,\n",
       "        0.13585023],\n",
       "       [0.1354755 , 0.1354755 , 0.1186882 , ..., 0.11881734, 0.11868817,\n",
       "        0.1354755 ],\n",
       "       ...,\n",
       "       [0.17582843, 0.19520215, 0.09854587, ..., 0.09893634, 0.09854664,\n",
       "        0.13585023],\n",
       "       [0.1354755 , 0.1354755 , 0.1186882 , ..., 0.11881734, 0.11868817,\n",
       "        0.1354755 ],\n",
       "       [0.17582843, 0.19520215, 0.09854587, ..., 0.09893634, 0.09854664,\n",
       "        0.13585023]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:06:23.106000Z",
     "iopub.status.busy": "2025-09-15T08:06:23.105723Z",
     "iopub.status.idle": "2025-09-15T08:06:23.111502Z",
     "shell.execute_reply": "2025-09-15T08:06:23.110625Z",
     "shell.execute_reply.started": "2025-09-15T08:06:23.105979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train['prediction_snorkel'] = lf_pred\n",
    "df_train['snorkel_confidence'] = lf_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:08:54.677004Z",
     "iopub.status.busy": "2025-09-15T08:08:54.676723Z",
     "iopub.status.idle": "2025-09-15T08:08:54.687381Z",
     "shell.execute_reply": "2025-09-15T08:08:54.686581Z",
     "shell.execute_reply.started": "2025-09-15T08:08:54.676984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train[\"snorkel_probs\"] = probs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:08:58.237086Z",
     "iopub.status.busy": "2025-09-15T08:08:58.236639Z",
     "iopub.status.idle": "2025-09-15T08:08:58.248894Z",
     "shell.execute_reply": "2025-09-15T08:08:58.248176Z",
     "shell.execute_reply.started": "2025-09-15T08:08:58.237066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prediction_snorkel</th>\n",
       "      <th>snorkel_confidence</th>\n",
       "      <th>snorkel_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>заказали 14.10.2017 , получили 25.10.2017 \\n\\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195202</td>\n",
       "      <td>[0.1758284279266772, 0.19520214640769284, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>футболка хорошего качества,но футболка не как ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195202</td>\n",
       "      <td>[0.1758284279266772, 0.19520214640769284, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>все отлично!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135475</td>\n",
       "      <td>[0.1354754974159609, 0.1354754974159609, 0.118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>рисунок не очень чёткий, а ткань прозрачная, в...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195202</td>\n",
       "      <td>[0.1758284279266772, 0.19520214640769284, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>плохо!!!низ рваный..деньги не вернули!открыла ...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.258021</td>\n",
       "      <td>[0.19997417589112063, 0.20251091383449046, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>спасибо,подошло по размеру.все понравилось в п...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135475</td>\n",
       "      <td>[0.1354754974159609, 0.1354754974159609, 0.118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>доставка быстрая, до саратова около 2 недель. ...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.258021</td>\n",
       "      <td>[0.19997417589112063, 0.20251091383449046, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>на внешний вид шапка нормальная, на большой об...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195202</td>\n",
       "      <td>[0.1758284279266772, 0.19520214640769284, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>за 4 месяца товар так и не дошел до покупателя.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135475</td>\n",
       "      <td>[0.1354754974159609, 0.1354754974159609, 0.118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>прислал пальто вместо куртки!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195202</td>\n",
       "      <td>[0.1758284279266772, 0.19520214640769284, 0.09...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1818 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  prediction_snorkel  \\\n",
       "0     заказали 14.10.2017 , получили 25.10.2017 \\n\\n...                   1   \n",
       "1     футболка хорошего качества,но футболка не как ...                   1   \n",
       "2                                        все отлично!!!                   0   \n",
       "3     рисунок не очень чёткий, а ткань прозрачная, в...                   1   \n",
       "4     плохо!!!низ рваный..деньги не вернули!открыла ...                   7   \n",
       "...                                                 ...                 ...   \n",
       "1813  спасибо,подошло по размеру.все понравилось в п...                   0   \n",
       "1814  доставка быстрая, до саратова около 2 недель. ...                   7   \n",
       "1815  на внешний вид шапка нормальная, на большой об...                   1   \n",
       "1816    за 4 месяца товар так и не дошел до покупателя.                   0   \n",
       "1817                    прислал пальто вместо куртки!!!                   1   \n",
       "\n",
       "      snorkel_confidence                                      snorkel_probs  \n",
       "0               0.195202  [0.1758284279266772, 0.19520214640769284, 0.09...  \n",
       "1               0.195202  [0.1758284279266772, 0.19520214640769284, 0.09...  \n",
       "2               0.135475  [0.1354754974159609, 0.1354754974159609, 0.118...  \n",
       "3               0.195202  [0.1758284279266772, 0.19520214640769284, 0.09...  \n",
       "4               0.258021  [0.19997417589112063, 0.20251091383449046, 0.0...  \n",
       "...                  ...                                                ...  \n",
       "1813            0.135475  [0.1354754974159609, 0.1354754974159609, 0.118...  \n",
       "1814            0.258021  [0.19997417589112063, 0.20251091383449046, 0.0...  \n",
       "1815            0.195202  [0.1758284279266772, 0.19520214640769284, 0.09...  \n",
       "1816            0.135475  [0.1354754974159609, 0.1354754974159609, 0.118...  \n",
       "1817            0.195202  [0.1758284279266772, 0.19520214640769284, 0.09...  \n",
       "\n",
       "[1818 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T18:36:04.444867Z",
     "iopub.status.busy": "2025-09-14T18:36:04.444513Z",
     "iopub.status.idle": "2025-09-14T18:36:04.451308Z",
     "shell.execute_reply": "2025-09-14T18:36:04.450402Z",
     "shell.execute_reply.started": "2025-09-14T18:36:04.444842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'доставка быстрая, до саратова около 2 недель. упакована качественно. сшита аккуратно, дефектов не обнаружила. ткань лёгкая, приятна к телу, немного (в меру) прозрачна. м на 44 размер.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[1814]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:09:13.015681Z",
     "iopub.status.busy": "2025-09-15T08:09:13.015356Z",
     "iopub.status.idle": "2025-09-15T08:09:13.033128Z",
     "shell.execute_reply": "2025-09-15T08:09:13.032292Z",
     "shell.execute_reply.started": "2025-09-15T08:09:13.015660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.to_parquet('snorkel_predictions.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Объединяем предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:11:24.397644Z",
     "iopub.status.busy": "2025-09-15T08:11:24.397386Z",
     "iopub.status.idle": "2025-09-15T08:11:24.435733Z",
     "shell.execute_reply": "2025-09-15T08:11:24.435180Z",
     "shell.execute_reply.started": "2025-09-15T08:11:24.397628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train_snorkel = pd.read_parquet('/kaggle/input/train-predictions/snorkel_predictions.parquet')\n",
    "df_train_llm = pd.read_parquet('/kaggle/input/train-predictions/llm_predictions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:14:07.871363Z",
     "iopub.status.busy": "2025-09-15T08:14:07.871089Z",
     "iopub.status.idle": "2025-09-15T08:14:07.876667Z",
     "shell.execute_reply": "2025-09-15T08:14:07.876098Z",
     "shell.execute_reply.started": "2025-09-15T08:14:07.871341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "p_snorkel = df_train_snorkel['snorkel_probs'].values\n",
    "p_snorkel = np.array(list(p_snorkel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:14:08.573611Z",
     "iopub.status.busy": "2025-09-15T08:14:08.573331Z",
     "iopub.status.idle": "2025-09-15T08:14:08.578549Z",
     "shell.execute_reply": "2025-09-15T08:14:08.577997Z",
     "shell.execute_reply.started": "2025-09-15T08:14:08.573591Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17582843, 0.19520215, 0.09854587, ..., 0.09893634, 0.09854664,\n",
       "        0.13585023],\n",
       "       [0.17582843, 0.19520215, 0.09854587, ..., 0.09893634, 0.09854664,\n",
       "        0.13585023],\n",
       "       [0.1354755 , 0.1354755 , 0.1186882 , ..., 0.11881734, 0.11868817,\n",
       "        0.1354755 ],\n",
       "       ...,\n",
       "       [0.17582843, 0.19520215, 0.09854587, ..., 0.09893634, 0.09854664,\n",
       "        0.13585023],\n",
       "       [0.1354755 , 0.1354755 , 0.1186882 , ..., 0.11881734, 0.11868817,\n",
       "        0.1354755 ],\n",
       "       [0.17582843, 0.19520215, 0.09854587, ..., 0.09893634, 0.09854664,\n",
       "        0.13585023]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:14:12.085419Z",
     "iopub.status.busy": "2025-09-15T08:14:12.084844Z",
     "iopub.status.idle": "2025-09-15T08:14:12.095031Z",
     "shell.execute_reply": "2025-09-15T08:14:12.094314Z",
     "shell.execute_reply.started": "2025-09-15T08:14:12.085397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "CLASSES = ['обувь','одежда','посуда','текстиль','товары для детей',\n",
    "           'украшения и аксессуары','электроника','нет товара']\n",
    "IDX = {c:i for i,c in enumerate(CLASSES)}\n",
    "NO_PRODUCT = IDX['нет товара']\n",
    "\n",
    "def build_llm_probs_from_label(lbls, confs, K=8):\n",
    "    \"\"\"\n",
    "    If you only have LLM hard labels + a confidence in [0,1], build a prob vector:\n",
    "    chosen = conf, the remaining mass is spread uniformly across others.\n",
    "    \"\"\"\n",
    "    P = np.full((len(lbls), K), 0.0, dtype=np.float32)\n",
    "    for i, (lab, conf) in enumerate(zip(lbls, confs)):\n",
    "        conf = float(conf)\n",
    "        conf = min(max(conf, 0.0), 1.0)\n",
    "        base = (1.0 - conf) / (K - 1 + 1e-9)\n",
    "        P[i, :] = base\n",
    "        P[i, IDX[lab]] = conf\n",
    "    return P\n",
    "\n",
    "def fuse_probs(p_snorkel, p_llm, llm_conf=None, tau_low=0.60,\n",
    "               alpha_min=0.20, alpha_max=0.80):\n",
    "    \"\"\"\n",
    "    p_snorkel: (N,8) Snorkel LabelModel probs\n",
    "    p_llm:     (N,8) LLM probs (from votes or build_llm_probs_from_label)\n",
    "    llm_conf:  (N,) optional scalar confidence per row; if None -> max(p_llm, axis=1)\n",
    "    tau_low:   if fused max prob < tau_low -> force 'нет товара'\n",
    "    alpha_*:   LLM weight bounds for per-example mixing\n",
    "    Returns: final_labels (N,), final_conf (N,), fused_probs (N,8)\n",
    "    \"\"\"\n",
    "    eps = 1e-9\n",
    "    # normalize & clip\n",
    "    pS = np.clip(p_snorkel, eps, 1.0)\n",
    "    pS /= pS.sum(1, keepdims=True)\n",
    "    pL = np.clip(p_llm, eps, 1.0)\n",
    "    pL /= pL.sum(1, keepdims=True)\n",
    "\n",
    "    # per-example LLM weight from its confidence\n",
    "    if llm_conf is None:\n",
    "        llm_conf = pL.max(1)\n",
    "    llm_conf = np.asarray(llm_conf, dtype=np.float32)\n",
    "    alpha = np.clip(alpha_min + (alpha_max - alpha_min) * llm_conf, alpha_min, alpha_max)\n",
    "    wL = alpha[:, None]\n",
    "    wS = (1.0 - alpha)[:, None]\n",
    "\n",
    "    # log-linear pooling (more robust than linear mixture)\n",
    "    logp = wL * np.log(pL) + wS * np.log(pS)\n",
    "    logp -= logp.max(1, keepdims=True)\n",
    "    p = np.exp(logp)\n",
    "    p /= p.sum(1, keepdims=True)\n",
    "\n",
    "    y = p.argmax(1)\n",
    "    conf = p.max(1)\n",
    "\n",
    "    # Low-confidence fallback → 'нет товара'\n",
    "    low = conf < float(tau_low)\n",
    "    y[low] = NO_PRODUCT\n",
    "    conf[low] = np.maximum(conf[low], tau_low)  # optional: floor for readability\n",
    "\n",
    "    return y, conf, p\n",
    "\n",
    "# Optional tiny guard: if text is service-only and fused conf is not high, force 'нет товара'\n",
    "SERVICE_RX = re.compile(r'(доставк|курьер|трек|спор|возврат|деньг|продавц|приложени|оплат)', re.I)\n",
    "def postprocess_service(texts, y, conf, p, tau_service=0.70):\n",
    "    for i, t in enumerate(texts):\n",
    "        if y[i] != NO_PRODUCT and conf[i] < tau_service and SERVICE_RX.search(t or \"\"):\n",
    "            y[i] = NO_PRODUCT\n",
    "    return y, conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:14:12.412597Z",
     "iopub.status.busy": "2025-09-15T08:14:12.412299Z",
     "iopub.status.idle": "2025-09-15T08:14:12.436417Z",
     "shell.execute_reply": "2025-09-15T08:14:12.435931Z",
     "shell.execute_reply.started": "2025-09-15T08:14:12.412576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "p_llm = build_llm_probs_from_label(df_train_llm[\"class_from_llm\"], df_train_llm[\"llm_confidence\"], K=len(CLASSES))\n",
    "y, conf, p = fuse_probs(p_snorkel, p_llm, llm_conf=df_train_llm[\"llm_confidence\"], tau_low=0.50)\n",
    "final_labels = [CLASSES[i] for i in y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:14:14.846207Z",
     "iopub.status.busy": "2025-09-15T08:14:14.845364Z",
     "iopub.status.idle": "2025-09-15T08:14:14.851138Z",
     "shell.execute_reply": "2025-09-15T08:14:14.850404Z",
     "shell.execute_reply.started": "2025-09-15T08:14:14.846180Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'одежда': 1130,\n",
       "         'нет товара': 598,\n",
       "         'обувь': 38,\n",
       "         'украшения и аксессуары': 18,\n",
       "         'текстиль': 28,\n",
       "         'товары для детей': 3,\n",
       "         'посуда': 3})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:14:19.160430Z",
     "iopub.status.busy": "2025-09-15T08:14:19.159931Z",
     "iopub.status.idle": "2025-09-15T08:14:19.164383Z",
     "shell.execute_reply": "2025-09-15T08:14:19.163847Z",
     "shell.execute_reply.started": "2025-09-15T08:14:19.160408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train['label'] = final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:14:22.414987Z",
     "iopub.status.busy": "2025-09-15T08:14:22.414421Z",
     "iopub.status.idle": "2025-09-15T08:14:22.428386Z",
     "shell.execute_reply": "2025-09-15T08:14:22.427791Z",
     "shell.execute_reply.started": "2025-09-15T08:14:22.414962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.to_parquet('train_with_labels.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерируем синтетику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-15T08:20:18.862151Z",
     "iopub.status.busy": "2025-09-15T08:20:18.861826Z",
     "iopub.status.idle": "2025-09-15T08:22:06.451566Z",
     "shell.execute_reply": "2025-09-15T08:22:06.450516Z",
     "shell.execute_reply.started": "2025-09-15T08:20:18.862129Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, accelerate\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.1\n",
      "    Uninstalling huggingface-hub-0.33.1:\n",
      "      Successfully uninstalled huggingface-hub-0.33.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.8.1\n",
      "    Uninstalling accelerate-1.8.1:\n",
      "      Successfully uninstalled accelerate-1.8.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.10.1 huggingface-hub-0.34.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tokenizers-0.22.0 transformers-4.56.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:22:06.453517Z",
     "iopub.status.busy": "2025-09-15T08:22:06.453267Z",
     "iopub.status.idle": "2025-09-15T08:22:06.460146Z",
     "shell.execute_reply": "2025-09-15T08:22:06.459237Z",
     "shell.execute_reply.started": "2025-09-15T08:22:06.453493Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['обувь',\n",
       " 'одежда',\n",
       " 'посуда',\n",
       " 'текстиль',\n",
       " 'товары для детей',\n",
       " 'украшения и аксессуары',\n",
       " 'электроника',\n",
       " 'нет товара']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-15T08:22:06.461547Z",
     "iopub.status.busy": "2025-09-15T08:22:06.460965Z",
     "iopub.status.idle": "2025-09-15T08:22:12.850462Z",
     "shell.execute_reply": "2025-09-15T08:22:12.849409Z",
     "shell.execute_reply.started": "2025-09-15T08:22:06.461518Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.47.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T09:04:48.421841Z",
     "iopub.status.busy": "2025-09-15T09:04:48.421297Z",
     "iopub.status.idle": "2025-09-15T09:05:46.782146Z",
     "shell.execute_reply": "2025-09-15T09:05:46.781221Z",
     "shell.execute_reply.started": "2025-09-15T09:04:48.421816Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038bfe13f05145adb68c1d50c4536dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "DTYPE = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_name = \"t-tech/T-lite-it-1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map={\"\": 0},   \n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_state_dict=True,\n",
    "    quantization_config=bnb_cfg,\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:28:52.987289Z",
     "iopub.status.busy": "2025-09-15T08:28:52.986426Z",
     "iopub.status.idle": "2025-09-15T08:28:53.328834Z",
     "shell.execute_reply": "2025-09-15T08:28:53.328274Z",
     "shell.execute_reply.started": "2025-09-15T08:28:52.987261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train_with_labels = pd.read_parquet('/kaggle/input/train-predictions/train_with_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:29:21.047084Z",
     "iopub.status.busy": "2025-09-15T08:29:21.046253Z",
     "iopub.status.idle": "2025-09-15T08:29:21.056018Z",
     "shell.execute_reply": "2025-09-15T08:29:21.055198Z",
     "shell.execute_reply.started": "2025-09-15T08:29:21.047056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train_with_labels = df_train_with_labels[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T09:05:46.784225Z",
     "iopub.status.busy": "2025-09-15T09:05:46.783847Z",
     "iopub.status.idle": "2025-09-15T09:05:46.795751Z",
     "shell.execute_reply": "2025-09-15T09:05:46.794840Z",
     "shell.execute_reply.started": "2025-09-15T09:05:46.784197Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151665, 3584, padding_idx=151643)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=151665, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- CONFIG ---\n",
    "import os, re, json, math, random, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Your 8 classes (labels already exist in df_train['label'])\n",
    "CLASSES = ['обувь','одежда','посуда','текстиль','товары для детей',\n",
    "           'украшения и аксессуары','электроника','нет товара']\n",
    "\n",
    "assert {'text','label'}.issubset(df_train_with_labels.columns), \"df_train must have columns: text, label\"\n",
    "\n",
    "# Make sure tokenizer/model are set for batched decoding\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:30:24.924379Z",
     "iopub.status.busy": "2025-09-15T08:30:24.923659Z",
     "iopub.status.idle": "2025-09-15T08:30:25.003154Z",
     "shell.execute_reply": "2025-09-15T08:30:25.002230Z",
     "shell.execute_reply.started": "2025-09-15T08:30:24.924350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 1) Mine top in-corpus n-grams per class (hints) ---\n",
    "def top_ngrams_for_class(df, label, topk=40, ngram_range=(1,2), min_df=2):\n",
    "    texts = df.loc[df['label']==label, 'text'].fillna(\"\").astype(str).tolist()\n",
    "    if len(texts) == 0:\n",
    "        return []\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, min_df=min_df, lowercase=True,\n",
    "                          token_pattern=r\"[A-Za-zА-Яа-я0-9ёЁ]+\")\n",
    "    X = vec.fit_transform(texts)\n",
    "    freqs = np.asarray(X.sum(axis=0)).ravel()\n",
    "    vocab = np.array(vec.get_feature_names_out())\n",
    "    # Drop overly generic tokens\n",
    "    keep = [i for i,t in enumerate(vocab) if len(t) >= 3]\n",
    "    idx = np.argsort(freqs[keep])[::-1][:topk]\n",
    "    return vocab[keep][idx].tolist()\n",
    "\n",
    "HINTS = {c: top_ngrams_for_class(df_train_with_labels, c, topk=40) for c in CLASSES}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-15T08:30:28.305186Z",
     "iopub.status.busy": "2025-09-15T08:30:28.304835Z",
     "iopub.status.idle": "2025-09-15T08:30:28.311410Z",
     "shell.execute_reply": "2025-09-15T08:30:28.310707Z",
     "shell.execute_reply.started": "2025-09-15T08:30:28.305165Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'обувь': ['размер',\n",
       "  'очень',\n",
       "  'все',\n",
       "  'рост',\n",
       "  'качество',\n",
       "  'носки',\n",
       "  'подошёл',\n",
       "  'товар',\n",
       "  'было',\n",
       "  'раз',\n",
       "  'отлично',\n",
       "  'после',\n",
       "  'пришёл',\n",
       "  'пришли',\n",
       "  'после первой',\n",
       "  'первой носки',\n",
       "  'первой',\n",
       "  'размеру',\n",
       "  'на фото',\n",
       "  'меня',\n",
       "  'как',\n",
       "  'доставка',\n",
       "  'быстро',\n",
       "  'больше',\n",
       "  'по размеру',\n",
       "  'хорошее',\n",
       "  'шел',\n",
       "  'что',\n",
       "  'хорошо',\n",
       "  'фото',\n",
       "  'цвет',\n",
       "  'месяц',\n",
       "  'материал на',\n",
       "  'материал',\n",
       "  'качество хорошее',\n",
       "  'хорошо но',\n",
       "  'заказывала',\n",
       "  'заказ',\n",
       "  'за 2',\n",
       "  'есть'],\n",
       " 'одежда': ['очень',\n",
       "  'размер',\n",
       "  'качество',\n",
       "  'как',\n",
       "  'что',\n",
       "  'доставка',\n",
       "  'ткань',\n",
       "  'платье',\n",
       "  'соответствует',\n",
       "  'все',\n",
       "  'так',\n",
       "  'спасибо',\n",
       "  'цвет',\n",
       "  'рост',\n",
       "  'заказала',\n",
       "  'быстро',\n",
       "  'пришла',\n",
       "  'фото',\n",
       "  'синтетика',\n",
       "  'хорошо',\n",
       "  'это',\n",
       "  'заказывала',\n",
       "  'продавец',\n",
       "  'рекомендую',\n",
       "  'быстрая',\n",
       "  'деньги',\n",
       "  'мне',\n",
       "  'как на',\n",
       "  'не соответствует',\n",
       "  'на фото',\n",
       "  'нитки',\n",
       "  'товар',\n",
       "  'материал',\n",
       "  'рукава',\n",
       "  'размеру',\n",
       "  'супер',\n",
       "  'хорошая',\n",
       "  'только',\n",
       "  'даже',\n",
       "  'торчат'],\n",
       " 'посуда': [],\n",
       " 'текстиль': ['ткань',\n",
       "  'очень',\n",
       "  'синтетика',\n",
       "  'только',\n",
       "  'качество',\n",
       "  'что',\n",
       "  'приятная',\n",
       "  'как',\n",
       "  'хороший',\n",
       "  'трусы',\n",
       "  'цвет',\n",
       "  'чем',\n",
       "  'цена',\n",
       "  'чуть',\n",
       "  'что на',\n",
       "  'совсем',\n",
       "  'стирки',\n",
       "  'телу',\n",
       "  'товар',\n",
       "  'фото',\n",
       "  'приятно',\n",
       "  'продавец',\n",
       "  'просто',\n",
       "  'после стирки',\n",
       "  'после',\n",
       "  'очень довольна',\n",
       "  'ночнушка',\n",
       "  'ниток',\n",
       "  'неприятная',\n",
       "  'приятная в',\n",
       "  'смотрится',\n",
       "  'не совсем',\n",
       "  'недели',\n",
       "  'не очень',\n",
       "  'на фото',\n",
       "  'колется',\n",
       "  'много',\n",
       "  'картинке',\n",
       "  'к телу',\n",
       "  'и не'],\n",
       " 'товары для детей': ['так', 'рост', 'и вес', 'вес'],\n",
       " 'украшения и аксессуары': ['очень',\n",
       "  'качество',\n",
       "  'стоит',\n",
       "  'яркие',\n",
       "  'слишком',\n",
       "  'раз',\n",
       "  'ощупь',\n",
       "  'чем',\n",
       "  'очень красивые',\n",
       "  'носить',\n",
       "  'на ощупь',\n",
       "  'не очень',\n",
       "  'красивые',\n",
       "  'как',\n",
       "  'выглядит',\n",
       "  'быстро'],\n",
       " 'электроника': [],\n",
       " 'нет товара': ['деньги',\n",
       "  'товар',\n",
       "  'вернули',\n",
       "  'продавец',\n",
       "  'заказ',\n",
       "  'очень',\n",
       "  'так',\n",
       "  'товар не',\n",
       "  'спор',\n",
       "  'пришел',\n",
       "  'и не',\n",
       "  'пришёл',\n",
       "  'не пришел',\n",
       "  'так и',\n",
       "  'качество',\n",
       "  'не пришёл',\n",
       "  'не вернули',\n",
       "  'месяца',\n",
       "  'деньги вернули',\n",
       "  'деньги не',\n",
       "  'все',\n",
       "  'заказ не',\n",
       "  'вернул',\n",
       "  'что',\n",
       "  'получила',\n",
       "  'спасибо',\n",
       "  'не получила',\n",
       "  'денег',\n",
       "  'ждала',\n",
       "  'быстро',\n",
       "  'как',\n",
       "  'рекомендую',\n",
       "  'соответствует',\n",
       "  'пришло',\n",
       "  'доставка',\n",
       "  'размер',\n",
       "  'открыла',\n",
       "  'продавца',\n",
       "  'открыла спор',\n",
       "  'нет']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T10:59:17.012839Z",
     "iopub.status.busy": "2025-09-15T10:59:17.012176Z",
     "iopub.status.idle": "2025-09-15T10:59:17.024870Z",
     "shell.execute_reply": "2025-09-15T10:59:17.024339Z",
     "shell.execute_reply.started": "2025-09-15T10:59:17.012816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PROMPT_EXAMPLES = {\n",
    "    'обувь': [\n",
    "        \"1. слишком долгая доставка, пахнут резиной\",\n",
    "        \"2. маленькие. когда одеваешь рисунка почти нет. белая подошва.\"\n",
    "    ],\n",
    "    'одежда': [\n",
    "        \"1. цвет черный а не темно синий , обидно. шапка хорошая.\"\n",
    "        \"2. на 2 размера меньше пришел , плохо прошит , полоски стерлись на следующий день , синтетик и не теплый\",\n",
    "        \"3. заказали 14.10.2017 , получили 25.10.2017 \\n\\nна мой размер 42, широкий как мешок. надо было все таки размер  s заказать. \\n\\nпо поводу качества хороший пуховик. \\n\\nмех натуральный , съемный. \\n\\nбуду продавать .\",\n",
    "        \"4. футболка хорошего качества,но футболка не как для девушек и женщин,а как на мужчину. она очень свободная. на свой м, заказала л. теперь не знаю что делать,ибо она мне велика, даже моему папе она полезет.\",\n",
    "        \"5. рисунок не очень чёткий, а ткань прозрачная, видно нижнее бельё\"\n",
    "    ],\n",
    "    'посуда': [\n",
    "        \"1. для тех, кто пьёт напитки литрами и не хочет постоянно отвлекаться на то, чтобы их делать.\",\n",
    "        \"2. вздумалось мне заказать на алиэкспресс стальную мисочку для лапши объёмом 1 000 мл. и вот заказ прибыл. нужно доставать и проверять.\"\n",
    "    ],\n",
    "    'текстиль': [\n",
    "        \"1. приятная, нежная ткань. хороший комплект. только цвет не такой сочный, как на картинке.\",\n",
    "        \"2. мое спасение! для ёрзающих во сне)))\",\n",
    "        \"3. теплый велюровый комплект постельного белья + дополнительные наволочки.\"\n",
    "    ],\n",
    "    'товары для детей': [\n",
    "        \"1. шапку покупала ребёнку из за пампона (он чёрный)так вот он совсем не держится на шапке!!!продавцу написала об этом но он игнорит!\",\n",
    "        \"2. для ребенка старше года бесполезная. для детей помладше или тех, что спят как статуи, возможно, сработает.\"\n",
    "    ],\n",
    "    'украшения и аксессуары': [\n",
    "        \"1. фигня, нитки торчат, хотела носить как пояс-баска, размер оказался огромный\",\n",
    "        \"2. эти серьги я покупала примерно в начале 2022 года. они потерялись, не отслеживались на территории рф - я была вынуждена открыть диспут и получить деньги обратно. \",\n",
    "        \"3. самые классные цепочки\"\n",
    "    ],\n",
    "    'электроника': [\n",
    "        \"1. так себе прибор для путешествий\",\n",
    "        \"2. недорогой спидометр, работает нормально\",\n",
    "        \"3. купил светодиодную лампу, прошло три дня, очень хорошо светит когда сидишь за столом,но иногда лампа перегревается но все норм или сидишь за компьютером, думаю ещё купить лампу другу на день рождение и себе вторую лампу, всем доволен, всем рекомендую\"\n",
    "    ],\n",
    "    'нет товара': [\n",
    "        \"1. все отлично!!!\",\n",
    "        \"2. доставка до спб 35 дней.\",\n",
    "        \"3. по началу всё отслеживалось, но в конце выяснилось, что пришло в балашиху, а заказывала в саратов! ждала, но заказ так и не пришёл, защита закончилась\",\n",
    "        \"4. заказ так и не пришёл, открыла спор, продавец его не принял, просил закрыть спор и подождать, пришлось привлекать алиэкспресс, возврат денег одобрили, деньги пришли.\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- 2) Prompt templates (Russian) ---\n",
    "GEN_SYSTEM = (\n",
    "    \"Ты генерируешь реалистичные отзывы для маркетплейса на русском языке. \"\n",
    "    \"Следуй классу и качеству формулировок корпуса. Без брендов, без ссылок. \"\n",
    "    \"Разнообразь стиль (полож./нейтр./негат.), избегай повторов. \"\n",
    "    \"Выводи строго JSONL: по одному объекту на строку. \"\n",
    "    \"Каждый объект: {\\\"text\\\": \\\"<отзыв без переносов>\\\", \\\"label\\\": \\\"<класс>\\\"}. \"\n",
    "    \"Не используй фигурные скобки внутри текста, без markdown и подсказок.\"\n",
    "    \"Примеры реальных отзывов:\"\n",
    "    )\n",
    "\n",
    "def make_gen_prompt(category: str, n_items: int, hints: list[str]) -> str:\n",
    "    # Compose concise hints (up to ~12 tokens/phrases)\n",
    "    sampled = \", \".join(random.sample(hints, k=min(12, max(3, len(hints))))) if hints else \"\"\n",
    "    user = (\n",
    "        f\"Сгенерируй {n_items} реалистичных отзывов категории «{category}». \"\n",
    "        \"Длина до 40 слов; варьируй стиль (положительный/нейтральный/негативный). \"\n",
    "        \"Если категория «нет товара», делай отзывы про доставку/продавца/деньги без упоминания конкретного товара. \"\n",
    "        f\"Подсказки корпуса: {sampled}\\n\"\n",
    "        \"Верни строго JSONL (одна строка — один объект) вида:\\n\"\n",
    "        \"{\\\"text\\\":\\\"...\\\", \\\"label\\\":\\\"\" + category + \"\\\"}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": GEN_SYSTEM + '\\n'.join(PROMPT_EXAMPLES[category])},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "    # chat→string\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# --- 3) Robust JSONL extraction ---\n",
    "def extract_jsonl_objects(text: str):\n",
    "    # Strip code fences if any\n",
    "    text = re.sub(r\"```(?:json)?\\s*|\\s*```\", \"\", text, flags=re.I)\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    objs = []\n",
    "    for ln in lines:\n",
    "        # quick filter: looks like a JSON object\n",
    "        if not (ln.startswith(\"{\") and ln.endswith(\"}\")):\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(ln)\n",
    "            if isinstance(obj, dict) and \"text\" in obj and \"label\" in obj:\n",
    "                objs.append(obj)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return objs\n",
    "\n",
    "# --- 4) Batched generation core ---\n",
    "def generate_batch(prompts: list[str],\n",
    "                   max_input_tokens=1000,\n",
    "                   max_new_tokens=1500,\n",
    "                   do_sample=True,\n",
    "                   temperature=0.8,\n",
    "                   top_p=0.9,\n",
    "                   repetition_penalty=1.05):\n",
    "    enc = tokenizer(\n",
    "        prompts, return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=max_input_tokens\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    # Slice new tokens only\n",
    "    input_lens = enc[\"attention_mask\"].sum(dim=1)\n",
    "    new_tokens = [out_ids[in_len:] for out_ids, in_len in zip(gen, input_lens)]\n",
    "    # Pad & decode\n",
    "    padded = pad_sequence(new_tokens, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    out_texts = tokenizer.batch_decode(padded, skip_special_tokens=True)\n",
    "    return out_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T10:01:17.471752Z",
     "iopub.status.busy": "2025-09-15T10:01:17.471302Z",
     "iopub.status.idle": "2025-09-15T10:01:17.476620Z",
     "shell.execute_reply": "2025-09-15T10:01:17.475967Z",
     "shell.execute_reply.started": "2025-09-15T10:01:17.471732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\nТы генерируешь реалистичные отзывы для маркетплейса на русском языке. Следуй классу и качеству формулировок корпуса. Без брендов, без ссылок. Разнообразь стиль (полож./нейтр./негат.), избегай повторов. Выводи строго JSONL: по одному объекту на строку. Каждый объект: {\"text\": \"<отзыв без переносов>\", \"label\": \"<класс>\"}. Не используй фигурные скобки внутри текста, без markdown и подсказок.Примеры реальных отзывов:1. супер прибор для путешествий\\n2. недорогой спидометр, работает нормально\\n3. купил светодиодную лампу, прошло три дня, очень хорошо светит когда сидишь за столом,но иногда лампа перегревается но все норм или сидишь за компьютером, думаю ещё купить лампу другу на день рождение и себе вторую лампу, всем доволен, всем рекомендую<|im_end|>\\n<|im_start|>user\\nСгенерируй 10 реалистичных отзывов категории «электроника». Длина до 80 слов; варьируй стиль (положительный/нейтральный/негативный). Если категория «нет товара», делай отзывы про доставку/продавца/деньги без упоминания конкретного товара. Подсказки корпуса: \\nВерни строго JSONL (одна строка — один объект) вида:\\n{\"text\":\"...\", \"label\":\"электроника\"}<|im_end|>\\n<|im_start|>assistant\\n']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = []\n",
    "prompts.append(make_gen_prompt(category='электроника', n_items=10, hints=HINTS.get('электроника', [])))\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T10:01:21.154031Z",
     "iopub.status.busy": "2025-09-15T10:01:21.153319Z",
     "iopub.status.idle": "2025-09-15T10:02:02.010000Z",
     "shell.execute_reply": "2025-09-15T10:02:02.009387Z",
     "shell.execute_reply.started": "2025-09-15T10:01:21.154005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "outs = generate_batch(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T10:02:02.011540Z",
     "iopub.status.busy": "2025-09-15T10:02:02.011259Z",
     "iopub.status.idle": "2025-09-15T10:02:02.016301Z",
     "shell.execute_reply": "2025-09-15T10:02:02.015748Z",
     "shell.execute_reply.started": "2025-09-15T10:02:02.011516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['```jsonl\\n{\"text\":\"У меня появился новый смартфон, и он превзошел все мои ожидания. Качество камеры просто потрясающее, а аккумулятор держит заряд целый день. Очень доволен покупкой.\", \"label\":\"электроника\"}\\n{\"text\":\"Приобрел ноутбук для работы и учебы. Он работает стабильно, но немного шумит при нагрузке. В целом, хорошая покупка для тех, кто ищет универсальное устройство.\", \"label\":\"электроника\"}\\n{\"text\":\"Этот Bluetooth-динамик отлично звучит, но батарея разряжается слишком быстро. Надеюсь, это временное явление, так как качество звука действительно впечатляет.\", \"label\":\"электроника\"}\\n{\"text\":\"Купил умные часы, они удобны в использовании, но периодически зависают. Надеюсь, обновление прошивки решит проблему.\", \"label\":\"электроника\"}\\n{\"text\":\"Светильник с LED-технологией светит ярко, но потребляет много электроэнергии. Для экономии стоит рассмотреть другие варианты.\", \"label\":\"электроника\"}\\n{\"text\":\"Приобрел умные колонки, которые стали центром моей домашней развлекательной системы. Звук отличный, но управление через голос не всегда понимает команды.\", \"label\":\"электроника\"}\\n{\"text\":\"Доставка электроники была задержана на неделю, хотя обещали быстрое обслуживание. Обратился в службу поддержки, и проблема была решена оперативно.\", \"label\":\"доставка\"}\\n{\"text\":\"Продавец был очень любезен и помог выбрать подходящую модель. Рекомендую его для покупок электроники.\", \"label\":\"продавец\"}\\n{\"text\":\"Не получил деньги за возврат электроники, так как продавец отказался принимать товар. Пришлось обращаться в банк для возврата средств.\", \"label\":\"деньги\"}\\n{\"text\":\"Приобрел умные наушники с отличным звуком, но через пару недель они перестали работать. Вернул товар, но был недоволен медленным процессом возврата.\", \"label\":\"электроника\"}\\n```']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T10:59:32.478902Z",
     "iopub.status.busy": "2025-09-15T10:59:32.478633Z",
     "iopub.status.idle": "2025-09-15T13:26:20.154288Z",
     "shell.execute_reply": "2025-09-15T13:26:20.153653Z",
     "shell.execute_reply.started": "2025-09-15T10:59:32.478881Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9796b55a940c469f8b8914e7cc33e33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen обувь:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791357e297784f98b261503169982161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen одежда:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e098b7cf5de347149671558d2ca09dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen посуда:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22513bb008614b749ccff415bea1da5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen текстиль:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e36a0f90842405a86e4bf9cea0c6197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen товары для детей:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7944d6e98aec4892a8a13d230cfa900f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen украшения и аксессуары:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f491b678399a4a2b93a43d1523ff8094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen электроника:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec2dc3e86154ea6868bd965b2e35807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen нет товара:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label provenance\n",
      "0  Моя первая покупка у этого продавца, и я остал...  обувь    llm_gen\n",
      "1  Получил заказ быстро, но обувь маломерит на од...  обувь    llm_gen\n",
      "2  Обувь отлично сидит по размеру, удобная и стил...  обувь    llm_gen\n",
      "3  На фото обувь выглядит лучше, чем в реальности...  обувь    llm_gen\n",
      "4  Пришла обувь в целости и сохранности, но мне о...  обувь    llm_gen \n",
      "Counts:\n",
      " label\n",
      "обувь                     50\n",
      "одежда                    50\n",
      "посуда                    50\n",
      "текстиль                  50\n",
      "товары для детей          50\n",
      "украшения и аксессуары    50\n",
      "электроника               50\n",
      "нет товара                50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5) Light QA filters for realism ---\n",
    "RX_CYR = re.compile(r\"[А-Яа-яЁё]\")\n",
    "def qa_keep(obj: dict, category: str):\n",
    "    txt = obj.get(\"text\",\"\").strip()\n",
    "    lbl = str(obj.get(\"label\",\"\")).strip().lower()\n",
    "    if lbl != category:\n",
    "        return False\n",
    "    # one line, reasonable length, Russian chars present\n",
    "    if (\"\\n\" in txt) or (len(txt.split()) < 12) or (len(txt.split()) > 90):\n",
    "        return False\n",
    "    if RX_CYR.search(txt) is None:\n",
    "        return False\n",
    "    # avoid trivial boilerplate\n",
    "    if len(set(txt.lower().split())) < 8:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# --- 6) Driver: generate N per class ---\n",
    "def generate_synthetics(df, target_per_class=250,\n",
    "                        prompts_per_call=2,\n",
    "                        items_per_prompt=20):\n",
    "    rows = []\n",
    "    seen = set()  # to dedupe by text\n",
    "    for category in CLASSES:\n",
    "        needed = target_per_class\n",
    "        pbar = tqdm(total=needed, desc=f\"Gen {category}\")\n",
    "        while needed > 0:\n",
    "            # Build a small batch of prompts\n",
    "            prompts = []\n",
    "            for _ in range(prompts_per_call):\n",
    "                k = min(items_per_prompt, needed)\n",
    "                prompts.append(make_gen_prompt(category, n_items=k, hints=HINTS.get(category, [])))\n",
    "            # Generate\n",
    "            outs = generate_batch(prompts)\n",
    "            # Parse & collect\n",
    "            got = 0\n",
    "            for out in outs:\n",
    "                objs = extract_jsonl_objects(out)\n",
    "                for obj in objs:\n",
    "                    if qa_keep(obj, category):\n",
    "                        t = obj[\"text\"].strip()\n",
    "                        if t not in seen:\n",
    "                            rows.append({\"text\": t, \"label\": category, \"provenance\": \"llm_gen\"})\n",
    "                            seen.add(t)\n",
    "                            got += 1\n",
    "                            pbar.update(1)\n",
    "                            needed -= 1\n",
    "                            if needed <= 0:\n",
    "                                break\n",
    "                if needed <= 0:\n",
    "                    break\n",
    "            # Small GC to keep mem tidy on Kaggle\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        pbar.close()\n",
    "    return pd.DataFrame(rows, columns=[\"text\",\"label\",\"provenance\"])\n",
    "\n",
    "# --- 7) Run generation ---\n",
    "df_synth = generate_synthetics(df_train, target_per_class=50,\n",
    "                               prompts_per_call=2,   # try 2–4 depending on VRAM\n",
    "                               items_per_prompt=5)  # 20 JSON objects per prompt\n",
    "print(df_synth.head(), \"\\nCounts:\\n\", df_synth['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T13:26:20.155580Z",
     "iopub.status.busy": "2025-09-15T13:26:20.155385Z",
     "iopub.status.idle": "2025-09-15T13:26:20.236822Z",
     "shell.execute_reply": "2025-09-15T13:26:20.236159Z",
     "shell.execute_reply.started": "2025-09-15T13:26:20.155565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_synth.to_parquet('synthetic_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T13:50:29.303443Z",
     "iopub.status.busy": "2025-09-15T13:50:29.303143Z",
     "iopub.status.idle": "2025-09-15T13:50:29.310366Z",
     "shell.execute_reply": "2025-09-15T13:50:29.309737Z",
     "shell.execute_reply.started": "2025-09-15T13:50:29.303420Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Очень довольны покупкой! Шапка идеально подходит по размеру, держится на голове ребёнка без проблем. Рекомендую!',\n",
       "       'Нейтрально. Покупали комбинезон для малыша. Подходит по возрасту, но хотелось бы более долговечный материал.',\n",
       "       'Разочарован покупкой. Несмотря на заявленный вес, кресло для автокресла оказалось слишком легким и неустойчивым. Не рекомендую.',\n",
       "       'Приобрели мягкую игрушку для годовалого сына. Вес и размер соответствуют описанию, но ребенок быстро потерял интерес.',\n",
       "       'Купили новый весовой набор для измерения роста и веса. Все отлично работает, точность высокая.',\n",
       "       'Ребёнок очень любит новую книжку с интерактивными элементами. Но срок службы аккумулятора оказался коротким.',\n",
       "       'Покупка детского стульчика для кормления оказалась разочарованием. Он неустойчивый и легко опрокидывается.',\n",
       "       'Шапка отлично держится на голове моего малыша! Материал приятный на ощупь и теплый. Рекомендую!',\n",
       "       'Коляски этого бренда удобны для наших размеров (вес 8 кг, рост 75 см), но нужно быть готовым к частым ремонтом.',\n",
       "       'Игрушка понравилась ребенку, но быстро потеряла свою привлекательность после нескольких дней игры. Нейтрально.',\n",
       "       'Пеленки удобные, но пришлось вернуть из-за несоответствия размеров (ребенок 3 месяца, вес 4 кг).',\n",
       "       'Набор развивающих игрушек для годовалого сына оказался слишком сложным. Сначала нужно разобраться самим.',\n",
       "       'Купили бутылочку для новорожденного, но она постоянно протекает. Продавец ответил, что это нормально.',\n",
       "       'Заказанный товар не соответствует описанию. Пришлось вернуть и купить у другого продавца.',\n",
       "       'Шапка для малыша оказалась идеальной! Мягкая, удобная и отлично сидит на голове. Ребёнок в восторге!',\n",
       "       'Купили для ребёнка кресло-балансир. Рост 120 см - идеально подходит. Ребёнок быстро привык и теперь часами проводит время на нём.',\n",
       "       'Покупали весы для контроля веса ребёнка. Очень точные, но дороговато. Пока довольны, но нужно было подумать перед покупкой.',\n",
       "       'Ростомер для дома установили недавно. Удобно следить за ростом ребёнка, но иногда показывает неточные значения.',\n",
       "       'Наконец-то нашел подходящую обувь для сына! Размер идеально подходит, и он чувствует себя комфортно. Рекомендую!',\n",
       "       'Покупал штанишки для девочки, но они оказались слишком тяжелыми для ее активности. Пожалею о покупке.',\n",
       "       'Качественная игрушка для малыша. Уже неделю не вылезает из рук, и это здорово развивает его моторику. Отличный выбор!',\n",
       "       'Купили велосипед для ребенка, но он оказался слишком тяжелым для него. Вернули продавцу, так как не подошел.',\n",
       "       'Шапка для ребёнка идеально села по размеру! Легкая и приятная на ощупь.',\n",
       "       'Новая кроватка для малыша понравилась своей прочностью, но ребёнок пока не оценил.',\n",
       "       'Детский стульчик для кормления оказался слишком тяжёлым для нашей квартиры. Покупкой не доволен.',\n",
       "       'Наконец-то нашла подходящий набор для развития мелкой моторики у малыша. Ребёнок в восторге!',\n",
       "       'Штаны для малыша идеально сидят по весу и росту, но мой сын вырос через неделю. Понадобилось замерять точно.',\n",
       "       'Детский матрас оказался слишком твердым для моего ребёнка. Теперь ищу более мягкую модель.',\n",
       "       'Покупка конструктора для двухлетки была успешной. Он с удовольствием собирал простые модели, но быстро потерял интерес.',\n",
       "       'Шапка для ребёнка идеально сидит! Материал приятный на ощупь и хорошо держит форму.',\n",
       "       'Покупали бутылочку для малыша, ростом 9 месяцев. Подошла отлично, но ребенок еще не может её держать самостоятельно.',\n",
       "       'Набор весов для младенцев точный, но цена завышена. Учитывая вес и возраст ребенка, можно найти лучше.',\n",
       "       'Шапка для малыша оказалась идеальной! Прекрасно сидит, удобная и теплая. Ребёнок в восторге!',\n",
       "       'Покупал комбинезон для сына. Размер подошёл, но качество ткани вызывает сомнения. Будем надеяться, что прослужит долго.',\n",
       "       'Накупили игрушек для двухлетнего ребёнка. Понравились яркие цвета и безопасность материалов. Но некоторые детали легко отваливаются.',\n",
       "       'Велосипед для девочки подошёл по размеру, но педали слишком большие для её ножек. Надо будет подумать о замене.',\n",
       "       'Мобиль для детской кроватки прекрасно выполнил свою функцию. Ребёнок спокойно засыпает под его звуки.',\n",
       "       'Покупка детского стола оказалась ошибочной. Материал не такой прочный, как на фото. Вернули деньги быстро.',\n",
       "       'Для годовалого ребёнка купили развивающую игрушку. Она оказалась слишком сложной для него. Вернули в магазин.',\n",
       "       'Купили детский матрасик. Удобный и поддерживающий, но стоимость выше, чем в других магазинах. Возможно, стоит поискать аналоги.',\n",
       "       'Покупали мягкую игрушку, но она развалилась через месяц использования. Вернули деньги, но это разочарование.',\n",
       "       'Купили детскую мебель, но она оказалась не такой прочной, как ожидали. Попробуем вернуть и выбрать другой вариант.',\n",
       "       'Игрушка для двухлетнего ребёнка понравилась. Яркая, интересная, но некоторые детали могут быть опасны для грызения.',\n",
       "       'Шапка для малыша очень понравилась! Мягкая и теплая, отлично держится на голове.',\n",
       "       'Купили развивающий кубик ребенку, но он быстро потерял интерес. Возможно, не подходит для активных детей.',\n",
       "       'Ребенок любит играть с мягкой игрушкой, но через неделю она развалилась. Качество оставляет желать лучшего.',\n",
       "       'Купили этот коврик для малыша - он очень прочный и удобный. Ребёнок уже ползает по нему с удовольствием.',\n",
       "       'Покупали бутылочку для новорожденного. Все отлично, кроме того, что она немного тяжелая для малыша. Но это мелочь.',\n",
       "       'Ожидали большего от этого ходунка - ребенок даже не пытался вставать на ножки, когда его поставили. Пожалеем потраченные деньги.',\n",
       "       'Купили этот стульчик для кормления - ребенок любит его, и мы с мужем довольны удобством использования.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_synth[df_synth['label'] == 'товары для детей']['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T10:36:08.494176Z",
     "iopub.status.busy": "2025-09-15T10:36:08.493565Z",
     "iopub.status.idle": "2025-09-15T10:36:08.499594Z",
     "shell.execute_reply": "2025-09-15T10:36:08.498964Z",
     "shell.execute_reply.started": "2025-09-15T10:36:08.494153Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Недавно заказала платье, но оно пришло с дефектом - одна нить вылезла. Обидно, так как продавец уверял в высоком качестве. Платье какое-то скромное, не соответствует описанию. Вернула и жду возврата денег.',\n",
       "       'Футболка идеально соответствует описанию: качество отличное, сшита аккуратно, цвет как на фото. Рекомендую!'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_synth[df_synth['label'] == 'одежда']['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T09:01:52.169169Z",
     "iopub.status.busy": "2025-09-15T09:01:52.168216Z",
     "iopub.status.idle": "2025-09-15T09:01:52.173182Z",
     "shell.execute_reply": "2025-09-15T09:01:52.172592Z",
     "shell.execute_reply.started": "2025-09-15T09:01:52.169142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"hf_device_map:\", getattr(model, \"hf_device_map\", \"n/a\"))\n",
    "try:\n",
    "    from accelerate import cpu_offload\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:41:44.186567Z",
     "iopub.status.busy": "2025-09-15T08:41:44.185760Z",
     "iopub.status.idle": "2025-09-15T08:41:44.192989Z",
     "shell.execute_reply": "2025-09-15T08:41:44.192152Z",
     "shell.execute_reply.started": "2025-09-15T08:41:44.186531Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers on CPU: 0\n"
     ]
    }
   ],
   "source": [
    "sum_cpu = sum(1 for _,p in model.named_parameters() if p.device.type != \"cuda\")\n",
    "print(\"Layers on CPU:\", sum_cpu)  # should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T06:27:10.629114Z",
     "iopub.status.busy": "2025-09-16T06:27:10.628524Z",
     "iopub.status.idle": "2025-09-16T06:27:13.610817Z",
     "shell.execute_reply": "2025-09-16T06:27:13.610132Z",
     "shell.execute_reply.started": "2025-09-16T06:27:10.629086Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ruwordnet in /usr/local/lib/python3.11/dist-packages (0.0.6)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.4)\n",
      "Requirement already satisfied: sqlalchemy<2.0 in /usr/local/lib/python3.11/dist-packages (from ruwordnet) (1.4.54)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.0->ruwordnet) (3.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ruwordnet nltk pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T06:27:13.612233Z",
     "iopub.status.busy": "2025-09-16T06:27:13.611963Z",
     "iopub.status.idle": "2025-09-16T06:27:14.709634Z",
     "shell.execute_reply": "2025-09-16T06:27:14.708938Z",
     "shell.execute_reply.started": "2025-09-16T06:27:13.612209Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading a ruwordnet model from https://github.com/avidale/python-ruwordnet/releases/download/0.0.4/ruwordnet-2021.db\n"
     ]
    }
   ],
   "source": [
    "!ruwordnet download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T06:29:46.637081Z",
     "iopub.status.busy": "2025-09-16T06:29:46.636397Z",
     "iopub.status.idle": "2025-09-16T06:29:46.805567Z",
     "shell.execute_reply": "2025-09-16T06:29:46.805008Z",
     "shell.execute_reply.started": "2025-09-16T06:29:46.637054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train_synth = pd.read_parquet('/kaggle/input/train-predictions/synthetic_data.parquet')\n",
    "df_train_with_labels = pd.read_parquet('/kaggle/input/train-predictions/train_with_labels.parquet')\n",
    "df_train_full = pd.concat([df_train_with_labels, df_train_synth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T06:30:51.209358Z",
     "iopub.status.busy": "2025-09-16T06:30:51.208531Z",
     "iopub.status.idle": "2025-09-16T06:30:51.313780Z",
     "shell.execute_reply": "2025-09-16T06:30:51.313057Z",
     "shell.execute_reply.started": "2025-09-16T06:30:51.209321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Text Augmentation Toolkit (RU/EN friendly) ===\n",
    "import re, random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ruwordnet import RuWordNet\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "_ = wn.ensure_loaded()\n",
    "morph = MorphAnalyzer()\n",
    "rwn = RuWordNet()\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Alphabets for char noise ---\n",
    "RU_LOWER = list(\"абвгдеёжзийклмнопрстуфхцчшщъыьэюя\")\n",
    "RU_UPPER = [c.upper() for c in RU_LOWER]\n",
    "EN_LOWER = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "EN_UPPER = [c.upper() for c in EN_LOWER]\n",
    "\n",
    "def _rand_other(char_list, cur):\n",
    "    if len(char_list) == 1: return cur\n",
    "    c = cur\n",
    "    while c == cur:\n",
    "        c = random.choice(char_list)\n",
    "    return c\n",
    "\n",
    "def char_noise(text: str, p_char: float = 0.02) -> str:\n",
    "    \"\"\"Replace each RU/EN letter with a random same-script, same-case letter with prob p_char.\"\"\"\n",
    "    out = []\n",
    "    for ch in text:\n",
    "        r = random.random()\n",
    "        if r >= p_char:\n",
    "            out.append(ch); continue\n",
    "        if ch in RU_LOWER:\n",
    "            out.append(_rand_other(RU_LOWER, ch))\n",
    "        elif ch in RU_UPPER:\n",
    "            out.append(_rand_other(RU_UPPER, ch))\n",
    "        elif ch in EN_LOWER:\n",
    "            out.append(_rand_other(EN_LOWER, ch))\n",
    "        elif ch in EN_UPPER:\n",
    "            out.append(_rand_other(EN_UPPER, ch))\n",
    "        else:\n",
    "            out.append(ch)  # don’t touch digits/punct/space\n",
    "    return \"\".join(out)\n",
    "\n",
    "RX_CYR = re.compile(r\"[А-Яа-яЁё]\")\n",
    "\n",
    "def _transfer_case(src: str, dst: str) -> str:\n",
    "    if src.isupper(): return dst.upper()\n",
    "    if src.istitle(): return dst.capitalize()\n",
    "    return dst\n",
    "\n",
    "def ru_synonyms(word: str, max_k: int = 5) -> list[str]:\n",
    "    \"\"\"Russian synonyms via RuWordNet (if available). Returns lemmas; tries to inflect back.\"\"\"\n",
    "    if rwn is None or morph is None: return []\n",
    "    p = morph.parse(word)[0]\n",
    "    lemma = p.normal_form\n",
    "    syns = set()\n",
    "    try:\n",
    "        # Collect lemmas from all synsets where lemma appears\n",
    "        for ss in rwn.get_synsets(lemma):\n",
    "            for lit in ss.literals:\n",
    "                cand = lit.text.lower()\n",
    "                if cand != lemma:\n",
    "                    syns.add(cand)\n",
    "    except Exception:\n",
    "        return []\n",
    "    syns = list(syns)[:max_k]\n",
    "    # try to inflect to the same grammemes\n",
    "    out = []\n",
    "    for s in syns:\n",
    "        sp = morph.parse(s)[0]\n",
    "        # attempt to match grammemes intersection\n",
    "        gramm = {g for g in p.tag.grammemes if g in sp.tag.grammemes}\n",
    "        try:\n",
    "            inf = sp.inflect(gramm) if gramm else None\n",
    "            out.append((inf.word if inf else s))\n",
    "        except Exception:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "def en_synonyms(word: str, max_k: int = 5) -> list[str]:\n",
    "    if wn is None: return []\n",
    "    wlow = word.lower()\n",
    "    syns = set()\n",
    "    try:\n",
    "        for ss in wn.synsets(wlow, lang='eng'):\n",
    "            for l in ss.lemmas(lang='eng'):\n",
    "                cand = l.name().replace(\"_\",\" \").lower()\n",
    "                if cand != wlow:\n",
    "                    syns.add(cand)\n",
    "    except Exception:\n",
    "        return []\n",
    "    return list(syns)[:max_k]\n",
    "\n",
    "def get_synonyms(word: str) -> list[str]:\n",
    "    \"\"\"Auto-detect RU/EN and fetch synonyms; may return [].\"\"\"\n",
    "    if RX_CYR.search(word):\n",
    "        return ru_synonyms(word)\n",
    "    else:\n",
    "        return en_synonyms(word)\n",
    "\n",
    "WORD_RX = re.compile(r\"[A-Za-zА-Яа-яЁё]+\")\n",
    "\n",
    "def synonym_replace(text: str, p_word: float = 0.12) -> str:\n",
    "    \"\"\"\n",
    "    With prob p_word per token, replace by a synonym (if available).\n",
    "    Keeps punctuation/spacing; preserves case shape of the original token.\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]+|\\s+\", text, flags=re.UNICODE)\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        if WORD_RX.fullmatch(tok) and random.random() < p_word:\n",
    "            syns = get_synonyms(tok)\n",
    "            syns = [s for s in syns if s.lower() != tok.lower()]\n",
    "            if syns:\n",
    "                repl = random.choice(syns)\n",
    "                out.append(_transfer_case(tok, repl))\n",
    "                continue\n",
    "        out.append(tok)\n",
    "    return \"\".join(out)\n",
    "\n",
    "def swap_adjacent_words(text: str, p_swap: float = 0.08) -> str:\n",
    "    \"\"\"\n",
    "    Swap adjacent word tokens with probability p_swap (non-overlapping).\n",
    "    Non-word tokens (punct/space) keep their positions relative to words.\n",
    "    \"\"\"\n",
    "    # Split into words and separators, then operate on word indices\n",
    "    parts = re.findall(r\"[A-Za-zА-Яа-яЁё]+|[^A-Za-zА-Яа-яЁё]+\", text)\n",
    "    # Identify word indices\n",
    "    idx = [i for i, p in enumerate(parts) if WORD_RX.fullmatch(p)]\n",
    "    i = 0\n",
    "    while i < len(idx) - 1:\n",
    "        if random.random() < p_swap:\n",
    "            a, b = idx[i], idx[i+1]\n",
    "            parts[a], parts[b] = parts[b], parts[a]\n",
    "            i += 2  # skip next to avoid overlapping swaps\n",
    "        else:\n",
    "            i += 1\n",
    "    return \"\".join(parts)\n",
    "\n",
    "# --- Top-level driver to build augmented DataFrame ---\n",
    "def augment_dataframe(df: pd.DataFrame,\n",
    "                      p_char: float = 0.03,\n",
    "                      p_syn: float = 0.2,\n",
    "                      p_swap: float = 0.2,\n",
    "                      which=(\"char_noise\",\"synonym\",\"swap\"),\n",
    "                      keep_original=True,\n",
    "                      repeat=1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply selected augmentations to each row of df[['text','label']].\n",
    "    Returns a new dataframe with columns: text, label, augmentation.\n",
    "    \"\"\"\n",
    "    assert {'text','label'}.issubset(df.columns)\n",
    "    rows = []\n",
    "    for i, (txt, lab) in tqdm(enumerate(df[['text','label']].itertuples(index=False))):\n",
    "        for _ in range(repeat):\n",
    "            text = \"\" if pd.isna(txt) else str(txt)\n",
    "            if \"swap\" in which:\n",
    "                text = swap_adjacent_words(text, p_swap)\n",
    "            if \"synonym\" in which:\n",
    "                text = synonym_replace(text, p_syn)\n",
    "            if \"char_noise\" in which:\n",
    "                text = char_noise(text, p_char)\n",
    "            rows.append({\"text\": text, \"label\": lab, \"augmentation\":\"true\"})\n",
    "        if keep_original:\n",
    "            rows.append({\"text\": txt, \"label\": lab, \"augmentation\":\"false\"})\n",
    "    return pd.DataFrame(rows, columns=[\"text\",\"label\",\"augmentation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T06:31:24.271155Z",
     "iopub.status.busy": "2025-09-16T06:31:24.270540Z",
     "iopub.status.idle": "2025-09-16T06:31:24.276261Z",
     "shell.execute_reply": "2025-09-16T06:31:24.275505Z",
     "shell.execute_reply.started": "2025-09-16T06:31:24.271122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train_full = df_train_full[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T06:31:26.965211Z",
     "iopub.status.busy": "2025-09-16T06:31:26.964882Z",
     "iopub.status.idle": "2025-09-16T06:31:40.569548Z",
     "shell.execute_reply": "2025-09-16T06:31:40.568782Z",
     "shell.execute_reply.started": "2025-09-16T06:31:26.965188Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2218it [00:13, 163.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text   label augmentation\n",
      "0  заказали 14.10.2017 , получили 25.10.2017 \\n\\n...  одежда         true\n",
      "1  голучили 14.10.2017 , заказали 25.10.2017 \\n\\n...  одежда         true\n",
      "2  получили 14.10.2017 , заказали 25.10.2017 \\n\\n...  одежда         true\n",
      "3  заказали 14.10.2017 , получили 25.10.2017 \\n\\n...  одежда        false\n",
      "4  футболка хорошего качества,но фугболка не как ...  одежда         true\n",
      "augmentation\n",
      "true     6654\n",
      "false    2218\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_augmented = augment_dataframe(df_train_full, p_char=0.03, p_syn=0.15, p_swap=0.2,\n",
    "                                 which=(\"char_noise\",\"synonym\",\"swap\"),\n",
    "                                 keep_original=True, repeat=3)\n",
    "print(df_augmented.head())\n",
    "print(df_augmented['augmentation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T06:32:17.903242Z",
     "iopub.status.busy": "2025-09-16T06:32:17.902950Z",
     "iopub.status.idle": "2025-09-16T06:32:17.908513Z",
     "shell.execute_reply": "2025-09-16T06:32:17.907886Z",
     "shell.execute_reply.started": "2025-09-16T06:32:17.903222Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['заказали 14.10.2017 , получили 25.10.2017 \\n\\nмой на рязмер 42, широкий как мешок. надо было все таки размер  s заказать. \\n\\nпо поводу хороший качества пуховик. \\n\\nмех съемный , натуральный. \\n\\nпродавать буду .',\n",
       "       'голучили 14.10.2017 , заказали 25.10.2017 \\n\\nмой на размер 42, широкий как мешок. надо было все таки размер  s заказать. \\n\\nпо поводу качества пуховик хоьосий. \\n\\nнатуральный мех , буду. \\n\\nсъемный продавать .',\n",
       "       'получили 14.10.2017 , заказали 25.10.2017 \\n\\nна мой широкий 42, ркзмюр кщк мешок. надо все было таки s  размср заказать. \\n\\nпо поводу хороший качестоа пуховик. \\n\\nнатуральный мех , съемный. \\n\\nпродавать буду .',\n",
       "       'заказали 14.10.2017 , получили 25.10.2017 \\n\\nна мой размер 42, широкий как мешок. надо было все таки размер  s заказать. \\n\\nпо поводу качества хороший пуховик. \\n\\nмех натуральный , съемный. \\n\\nбуду продавать .',\n",
       "       'футболка хорошего качества,но фугболка не как девушек для и женщин,как а мужчину на. она очень свободная. свой на ч, ыаказала л. теперь знаю не ото делать,ибо она мне всника, моему даже папе она полезет.',\n",
       "       'аорошего футболка качества,футболка но вн как для девушек женщин и,как а на мужчину. она свободная кчень. на м свой, заназала тепера. л ёнаю не что делать,ибо она мне велика, моему даже она папе полезет.',\n",
       "       'футболка качества хорошего,но футболка не как еля девуюек и женщин,а кък на мужчину. она своблдная ожень. на свой м, заказала л. теперь не знаю делать что,ибо она велика мне, даже папе моему она нолезет.',\n",
       "       'футболка хорошего качества,но футболка не как для девушек и женщин,а как на мужчину. она очень свободная. на свой м, заказала л. теперь не знаю что делать,ибо она мне велика, даже моему папе она полезет.',\n",
       "       'все отлиьно!!!', 'все отлично!!!'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_augmented.head(10)['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T06:35:08.839889Z",
     "iopub.status.busy": "2025-09-16T06:35:08.839359Z",
     "iopub.status.idle": "2025-09-16T06:35:08.866484Z",
     "shell.execute_reply": "2025-09-16T06:35:08.865962Z",
     "shell.execute_reply.started": "2025-09-16T06:35:08.839864Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_augmented.to_parquet('train_augmented.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-17T04:20:13.052736Z",
     "iopub.status.busy": "2025-09-17T04:20:13.052040Z",
     "iopub.status.idle": "2025-09-17T04:21:47.252545Z",
     "shell.execute_reply": "2025-09-17T04:21:47.251826Z",
     "shell.execute_reply.started": "2025-09-17T04:20:13.052709Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.44.2\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trl==0.9.6\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft==0.11.1\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate==0.33.0\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting bitsandbytes==0.44.1\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.5.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.2)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (2.6.0+cu124)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (3.6.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.9.6)\n",
      "  Downloading tyro-0.9.31-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (7.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.4.0->trl==0.9.6)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.4.0->trl==0.9.6)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.4.0->trl==0.9.6)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4.0->trl==0.9.6)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.4.0->trl==0.9.6)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.4.0->trl==0.9.6)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.4.0->trl==0.9.6)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.4.0->trl==0.9.6)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.4.0->trl==0.9.6)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.4.0->trl==0.9.6)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4.0->trl==0.9.6) (1.3.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.9.6) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.9.6) (14.0.0)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.9.6)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.9.6) (4.4.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (0.70.16)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2025.6.15)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (3.12.13)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (2.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4.0->trl==0.9.6) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.44.2) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.44.2) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.44.2) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.44.2) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.9.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (1.20.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.44.2) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.9.6) (1.17.0)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading tyro-0.9.31-py3-none-any.whl (131 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.7/131.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: shtab, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tyro, tokenizers, nvidia-cusolver-cu12, transformers, accelerate, trl, peft, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.8.1\n",
      "    Uninstalling accelerate-1.8.1:\n",
      "      Successfully uninstalled accelerate-1.8.1\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.15.2\n",
      "    Uninstalling peft-0.15.2:\n",
      "      Successfully uninstalled peft-0.15.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.33.0 bitsandbytes-0.44.1 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.11.1 shtab-1.7.2 tokenizers-0.19.1 transformers-4.44.2 trl-0.9.6 tyro-0.9.31\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"transformers==4.44.2\" \"trl==0.9.6\" \"peft==0.11.1\" \"accelerate==0.33.0\" \"bitsandbytes==0.44.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:21:47.254118Z",
     "iopub.status.busy": "2025-09-17T04:21:47.253845Z",
     "iopub.status.idle": "2025-09-17T04:21:47.257838Z",
     "shell.execute_reply": "2025-09-17T04:21:47.257269Z",
     "shell.execute_reply.started": "2025-09-17T04:21:47.254092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:21:47.258825Z",
     "iopub.status.busy": "2025-09-17T04:21:47.258586Z",
     "iopub.status.idle": "2025-09-17T04:21:51.428711Z",
     "shell.execute_reply": "2025-09-17T04:21:51.428106Z",
     "shell.execute_reply.started": "2025-09-17T04:21:47.258803Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.44.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:21:51.430957Z",
     "iopub.status.busy": "2025-09-17T04:21:51.430591Z",
     "iopub.status.idle": "2025-09-17T04:21:51.437958Z",
     "shell.execute_reply": "2025-09-17T04:21:51.437065Z",
     "shell.execute_reply.started": "2025-09-17T04:21:51.430938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.6'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import trl\n",
    "trl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:21:51.438997Z",
     "iopub.status.busy": "2025-09-17T04:21:51.438737Z",
     "iopub.status.idle": "2025-09-17T04:21:51.608881Z",
     "shell.execute_reply": "2025-09-17T04:21:51.608333Z",
     "shell.execute_reply.started": "2025-09-17T04:21:51.438972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_augmented = pd.read_parquet('/kaggle/input/train-predictions/train_augmented.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:21:51.609831Z",
     "iopub.status.busy": "2025-09-17T04:21:51.609606Z",
     "iopub.status.idle": "2025-09-17T04:21:51.649459Z",
     "shell.execute_reply": "2025-09-17T04:21:51.648919Z",
     "shell.execute_reply.started": "2025-09-17T04:21:51.609813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 8872 → New size: 4436\n",
      "Original dist:\n",
      " label\n",
      "нет товара                0.292155\n",
      "обувь                     0.039675\n",
      "одежда                    0.532011\n",
      "посуда                    0.023895\n",
      "текстиль                  0.035167\n",
      "товары для детей          0.023895\n",
      "украшения и аксессуары    0.030658\n",
      "электроника               0.022543\n",
      "Name: proportion, dtype: float64\n",
      "New dist:\n",
      " label\n",
      "нет товара                0.292155\n",
      "обувь                     0.039675\n",
      "одежда                    0.532011\n",
      "посуда                    0.023895\n",
      "текстиль                  0.035167\n",
      "товары для детей          0.023895\n",
      "украшения и аксессуары    0.030658\n",
      "электроника               0.022543\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def stratified_fraction(df: pd.DataFrame, label_col: str = \"label\",\n",
    "                        frac: float = 0.5, random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a stratified subsample with size ≈ frac * len(df), preserving label fractions.\n",
    "    Ensures the total is exactly round(frac * N) via fractional rounding.\n",
    "    \"\"\"\n",
    "    assert 0 < frac < 1, \"frac must be in (0,1)\"\n",
    "    assert label_col in df.columns, f\"'{label_col}' column not found\"\n",
    "\n",
    "    counts = df[label_col].value_counts()\n",
    "    desired = counts * frac\n",
    "    target_total = int(round(len(df) * frac))\n",
    "\n",
    "    floors = np.floor(desired).astype(int)\n",
    "    remainder = target_total - floors.sum()\n",
    "\n",
    "    targets = floors.copy()\n",
    "\n",
    "    if remainder > 0:\n",
    "        # Give +1 to classes with largest fractional part\n",
    "        fractional = (desired - floors).sort_values(ascending=False)\n",
    "        for lab in fractional.index[:remainder]:\n",
    "            targets[lab] += 1\n",
    "    elif remainder < 0:\n",
    "        # Remove 1 from classes with smallest fractional part (but not below 0)\n",
    "        fractional = (desired - floors).sort_values(ascending=True)\n",
    "        removed = 0\n",
    "        for lab in fractional.index:\n",
    "            if removed == -remainder:\n",
    "                break\n",
    "            if targets[lab] > 0:\n",
    "                targets[lab] -= 1\n",
    "                removed += 1\n",
    "\n",
    "    # Sample per class\n",
    "    parts = []\n",
    "    for lab, n in targets.items():\n",
    "        if n <= 0:\n",
    "            continue\n",
    "        grp = df[df[label_col] == lab]\n",
    "        # If a class is tiny and rounding asked for more than available (shouldn't happen), clip:\n",
    "        n = min(n, len(grp))\n",
    "        parts.append(grp.sample(n=n, random_state=random_state))\n",
    "\n",
    "    df_out = pd.concat(parts, axis=0).sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    # Sanity checks (optional)\n",
    "    print(\"Original size:\", len(df), \"→ New size:\", len(df_out))\n",
    "    print(\"Original dist:\\n\", (df[label_col].value_counts(normalize=True)).sort_index())\n",
    "    print(\"New dist:\\n\", (df_out[label_col].value_counts(normalize=True)).sort_index())\n",
    "\n",
    "    return df_out\n",
    "\n",
    "df_augmented = stratified_fraction(df_augmented, label_col=\"label\", frac=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:21:51.650346Z",
     "iopub.status.busy": "2025-09-17T04:21:51.650104Z",
     "iopub.status.idle": "2025-09-17T04:22:08.959878Z",
     "shell.execute_reply": "2025-09-17T04:22:08.959292Z",
     "shell.execute_reply.started": "2025-09-17T04:21:51.650328Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 04:21:56.448854: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758082916.648834      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758082916.713495      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# ===================== QLoRA single-token classifier (Weighted F1) =====================\n",
    "# Requirements: transformers>=4.43, peft>=0.11, trl>=0.9, bitsandbytes>=0.43, scikit-learn\n",
    "import os, re, json, random, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "                          LogitsProcessor)\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:22:08.960961Z",
     "iopub.status.busy": "2025-09-17T04:22:08.960478Z",
     "iopub.status.idle": "2025-09-17T04:22:08.983854Z",
     "shell.execute_reply": "2025-09-17T04:22:08.983262Z",
     "shell.execute_reply.started": "2025-09-17T04:22:08.960943Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3770 Eval size: 666\n",
      "Train class counts:\n",
      " label\n",
      "одежда                    2006\n",
      "нет товара                1101\n",
      "обувь                      149\n",
      "текстиль                   133\n",
      "украшения и аксессуары     116\n",
      "посуда                      90\n",
      "товары для детей            90\n",
      "электроника                 85\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Setup & constants --------------------\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "BASE_MODEL = \"t-tech/T-lite-it-1.0\"  # your T-Lite checkpoint\n",
    "CLASSES = ['обувь','одежда','посуда','текстиль','товары для детей',\n",
    "           'украшения и аксессуары','электроника','нет товара']\n",
    "IDX = {c:i for i,c in enumerate(CLASSES)}\n",
    "\n",
    "# Ensure df_augmented has the right columns\n",
    "assert {'text','label'}.issubset(df_augmented.columns), \"df_augmented must have 'text' and 'label'\"\n",
    "\n",
    "# -------------------- Train/val split (stratified) --------------------\n",
    "df_augmented = df_augmented.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "train_df, val_df = train_test_split(\n",
    "    df_augmented[['text','label']].dropna(),\n",
    "    test_size=0.15, random_state=42, stratify=df_augmented['label']\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df   = val_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"Eval size:\", len(val_df))\n",
    "print(\"Train class counts:\\n\", train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:22:08.984707Z",
     "iopub.status.busy": "2025-09-17T04:22:08.984479Z",
     "iopub.status.idle": "2025-09-17T04:22:11.987890Z",
     "shell.execute_reply": "2025-09-17T04:22:11.987029Z",
     "shell.execute_reply.started": "2025-09-17T04:22:08.984687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip -q install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:22:11.991454Z",
     "iopub.status.busy": "2025-09-17T04:22:11.991246Z",
     "iopub.status.idle": "2025-09-17T04:22:14.202779Z",
     "shell.execute_reply": "2025-09-17T04:22:14.202098Z",
     "shell.execute_reply.started": "2025-09-17T04:22:11.991435Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a96d58b3604d90846bd253adcb13b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167d1a931fef48ca8e9362033cfa0f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f43a861aafc43dbb0e68cfe4b8b36fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a23c08c873245b5adadb535f4af4026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------- Tokenizer & label tokens --------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"  # better for decoder-only\n",
    "\n",
    "# One special token per class; model will learn to generate exactly this single token\n",
    "label_to_token = {c: f\"<cat_{i}>\" for i, c in enumerate(CLASSES)}\n",
    "token_to_label = {v: k for k, v in label_to_token.items()}\n",
    "specials = list(label_to_token.values())\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": specials})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:22:14.203659Z",
     "iopub.status.busy": "2025-09-17T04:22:14.203433Z",
     "iopub.status.idle": "2025-09-17T04:22:14.222053Z",
     "shell.execute_reply": "2025-09-17T04:22:14.221400Z",
     "shell.execute_reply.started": "2025-09-17T04:22:14.203642Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Заказ был отправлен месяц но, назад до сих пор...</td>\n",
       "      <td>нет товара</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>цвет совсем не такой как был заказан</td>\n",
       "      <td>нет товара</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>качество на троечку</td>\n",
       "      <td>нет товара</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>шорты пбдошли размером на м  кг 48 вес, рост 1...</td>\n",
       "      <td>одежда</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>получить Ржидал красисую миску, но пришла она ...</td>\n",
       "      <td>посуда</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3765</th>\n",
       "      <td>товар не пишел. никакого общенияс продавцом не...</td>\n",
       "      <td>нет товара</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3766</th>\n",
       "      <td>заказ пнишел через месяц до окутска. классный ...</td>\n",
       "      <td>нет товара</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767</th>\n",
       "      <td>ну такое. в целом неплохо. на тдоечку за ьакие...</td>\n",
       "      <td>нет товара</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3768</th>\n",
       "      <td>кяута классная. пришла без размера. заказывала...</td>\n",
       "      <td>одежда</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>тонкая приятеац пояс. ткань юе анатомически вы...</td>\n",
       "      <td>одежда</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3770 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text       label\n",
       "0     Заказ был отправлен месяц но, назад до сих пор...  нет товара\n",
       "1                  цвет совсем не такой как был заказан  нет товара\n",
       "2                                   качество на троечку  нет товара\n",
       "3     шорты пбдошли размером на м  кг 48 вес, рост 1...      одежда\n",
       "4     получить Ржидал красисую миску, но пришла она ...      посуда\n",
       "...                                                 ...         ...\n",
       "3765  товар не пишел. никакого общенияс продавцом не...  нет товара\n",
       "3766  заказ пнишел через месяц до окутска. классный ...  нет товара\n",
       "3767  ну такое. в целом неплохо. на тдоечку за ьакие...  нет товара\n",
       "3768  кяута классная. пришла без размера. заказывала...      одежда\n",
       "3769  тонкая приятеац пояс. ткань юе анатомически вы...      одежда\n",
       "\n",
       "[3770 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:22:14.222912Z",
     "iopub.status.busy": "2025-09-17T04:22:14.222702Z",
     "iopub.status.idle": "2025-09-17T04:22:14.229341Z",
     "shell.execute_reply": "2025-09-17T04:22:14.228658Z",
     "shell.execute_reply.started": "2025-09-17T04:22:14.222895Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------- Prompt & tokenization --------------------\n",
    "SYSTEM_INSTR = (\n",
    "    \"Ты — точный классификатор отзывов маркетплейса. Выбери ровно один ярлык \"\n",
    "    \"из списка. Если отзыв про доставку/продавца/деньги/спор без явного товара — метка «нет товара». \"\n",
    "    \"Отвечай только одним служебным токеном класса.\"\n",
    ")\n",
    "\n",
    "def build_chat_prompt(text: str) -> str:\n",
    "    labels_str = \", \".join([f'{label_to_token[c]}({c})' for c in CLASSES])\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_INSTR},\n",
    "        {\"role\": \"user\", \"content\": f\"Отзыв:\\n{text}\\n\\nМетки: {labels_str}\"},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "def tokenize_row(ex):\n",
    "    # Format as: [prompt][answer_token]; mask all prompt tokens, supervise only the last token\n",
    "    txt = \"\" if ex[\"text\"] is None else str(ex[\"text\"])\n",
    "    ytok = label_to_token[ex[\"label\"]]\n",
    "    prompt = build_chat_prompt(txt)\n",
    "    full   = prompt + ytok\n",
    "    enc = tokenizer(full, truncation=True, max_length=512)   # no padding here\n",
    "    L = len(enc[\"input_ids\"])\n",
    "    enc[\"labels\"] = [-100]*(L-1) + [enc[\"input_ids\"][-1]]    # 1 label per token, last is supervised\n",
    "    # ensure plain Python lists of ints (no numpy/ragged objs)\n",
    "    for k in (\"input_ids\",\"attention_mask\",\"labels\"):\n",
    "        enc[k] = list(map(int, enc[k]))\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:22:14.230367Z",
     "iopub.status.busy": "2025-09-17T04:22:14.230141Z",
     "iopub.status.idle": "2025-09-17T04:22:24.568095Z",
     "shell.execute_reply": "2025-09-17T04:22:24.567280Z",
     "shell.execute_reply.started": "2025-09-17T04:22:14.230352Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6674327a9ae4707899974cbfdcf5d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize train:   0%|          | 0/3770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9307be609845f59c4a3fecd16bec58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize val:   0%|          | 0/666 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df   = val_df.reset_index(drop=True)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[\"text\",\"label\"]])\n",
    "val_ds   = Dataset.from_pandas(val_df[[\"text\",\"label\"]])\n",
    "\n",
    "train_tok = train_ds.map(tokenize_row, remove_columns=train_ds.column_names, desc=\"Tokenize train\")\n",
    "val_tok   = val_ds.map(tokenize_row,   remove_columns=val_ds.column_names,   desc=\"Tokenize val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:22:24.569725Z",
     "iopub.status.busy": "2025-09-17T04:22:24.569068Z",
     "iopub.status.idle": "2025-09-17T04:22:24.573293Z",
     "shell.execute_reply": "2025-09-17T04:22:24.572640Z",
     "shell.execute_reply.started": "2025-09-17T04:22:24.569706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds = DatasetDict(train=train_tok, validation=val_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:22:24.574283Z",
     "iopub.status.busy": "2025-09-17T04:22:24.574051Z",
     "iopub.status.idle": "2025-09-17T04:22:24.605325Z",
     "shell.execute_reply": "2025-09-17T04:22:24.604701Z",
     "shell.execute_reply.started": "2025-09-17T04:22:24.574258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# sanity: lengths match and are 1-D\n",
    "def _check(ds, n=5):\n",
    "    for i in range(min(n, len(ds))):\n",
    "        L = len(ds[i][\"input_ids\"])\n",
    "        assert len(ds[i][\"labels\"]) == L and isinstance(ds[i][\"labels\"][-1], int)\n",
    "_check(train_tok)\n",
    "_check(val_tok)\n",
    "\n",
    " # custom collator that pads inputs AND labels together\n",
    "def collate_fn(batch):\n",
    "     pad_id = tokenizer.pad_token_id\n",
    "     max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "     input_ids, attn_mask, labels = [], [], []\n",
    "     for ex in batch:\n",
    "         L = len(ex[\"input_ids\"]); pad_len = max_len - L\n",
    "         input_ids.append(torch.tensor(ex[\"input_ids\"] + [pad_id]*pad_len, dtype=torch.long))\n",
    "         attn_mask.append(torch.tensor(ex[\"attention_mask\"] + [0]*pad_len, dtype=torch.long))\n",
    "         labels.append(torch.tensor(ex[\"labels\"] + [-100]*pad_len, dtype=torch.long))  # <- pad labels\n",
    "     return {\n",
    "         \"input_ids\": torch.stack(input_ids, 0),\n",
    "         \"attention_mask\": torch.stack(attn_mask, 0),\n",
    "         \"labels\": torch.stack(labels, 0),\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:22:24.606210Z",
     "iopub.status.busy": "2025-09-17T04:22:24.606017Z",
     "iopub.status.idle": "2025-09-17T04:22:37.419192Z",
     "shell.execute_reply": "2025-09-17T04:22:37.418268Z",
     "shell.execute_reply.started": "2025-09-17T04:22:24.606195Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting triton==3.0.0\n",
      "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton==3.0.0) (3.18.0)\n",
      "Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires triton==3.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 3.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"triton==3.0.0\"\n",
    "   # installs a recent compatible Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:22:37.420569Z",
     "iopub.status.busy": "2025-09-17T04:22:37.420265Z",
     "iopub.status.idle": "2025-09-17T04:22:37.424687Z",
     "shell.execute_reply": "2025-09-17T04:22:37.424062Z",
     "shell.execute_reply.started": "2025-09-17T04:22:37.420512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import gc, torch, sys\n",
    "\n",
    "# # 1) Delete big refs you created earlier\n",
    "# for name in [\"trainer\",\"model\",\"base\",\"tok_fast\",\"tok_slow\",\"enc\",\"gen\",\"ds\",\"train_tok\",\"val_tok\"]:\n",
    "#     if name in globals():\n",
    "#         try:\n",
    "#             del globals()[name]\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "# # 2) Collect garbage & free CUDA cache\n",
    "# gc.collect()\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "#     torch.cuda.ipc_collect()\n",
    "\n",
    "# # 3) Quick memory check (optional)\n",
    "# if torch.cuda.is_available():\n",
    "#     used = torch.cuda.memory_allocated()/1024**3\n",
    "#     reserved = torch.cuda.memory_reserved()/1024**3\n",
    "#     print(f\"After cleanup — used: {used:.2f} GB, reserved: {reserved:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:22:37.425719Z",
     "iopub.status.busy": "2025-09-17T04:22:37.425446Z",
     "iopub.status.idle": "2025-09-17T04:25:36.376255Z",
     "shell.execute_reply": "2025-09-17T04:25:36.375636Z",
     "shell.execute_reply.started": "2025-09-17T04:22:37.425701Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb3559cbf6845b48cb276586c6fd7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/712 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44973ca0a0842ff943e46bc63326055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8dae57609443eb90abf33128a6e7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3e0ea561d041608782966730548be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9f175d1b97438c904d9009a65ed56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29955a3ddd7d49559e70413aa8567cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a91af03fba4753bda35f8f54077bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aef6fa46089437ea7c5c55311e80d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a785768d584e47218472df3dca1ea5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# -------------------- Load 4-bit base & wrap with LoRA --------------------\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                             bnb_4bit_compute_dtype=DTYPE,\n",
    "                             bnb_4bit_use_double_quant=True,\n",
    "                             bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_cfg,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map={\"\": 0} if torch.cuda.is_available() else \"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "# Resize embeddings to include new class tokens\n",
    "base.resize_token_embeddings(len(tokenizer))\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "\n",
    "peft_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],  # common for LLaMA/Mistral-like\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = get_peft_model(base, peft_cfg)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:25:36.377252Z",
     "iopub.status.busy": "2025-09-17T04:25:36.377061Z",
     "iopub.status.idle": "2025-09-17T04:25:36.385319Z",
     "shell.execute_reply": "2025-09-17T04:25:36.384787Z",
     "shell.execute_reply.started": "2025-09-17T04:25:36.377236Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 7,653,184,000 || trainable%: 0.5275\n",
      "Any trainable params? True\n"
     ]
    }
   ],
   "source": [
    "# Optional but helpful diagnostics\n",
    "model.print_trainable_parameters()  # should show a nonzero % trainable\n",
    "print(\"Any trainable params?\", any(p.requires_grad for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:25:36.386362Z",
     "iopub.status.busy": "2025-09-17T04:25:36.386146Z",
     "iopub.status.idle": "2025-09-17T04:25:37.490568Z",
     "shell.execute_reply": "2025-09-17T04:25:37.489984Z",
     "shell.execute_reply.started": "2025-09-17T04:25:36.386346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(ds[\"train\"])):\n",
    "    ex = ds[\"train\"][i]\n",
    "    if not (isinstance(ex[\"labels\"], list) and len(ex[\"labels\"]) == len(ex[\"input_ids\"])\n",
    "            and all(isinstance(x, int) for x in ex[\"labels\"])):\n",
    "        print(\"Bad row:\", i, type(ex[\"labels\"]), len(ex[\"labels\"]), len(ex[\"input_ids\"]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T04:25:37.491491Z",
     "iopub.status.busy": "2025-09-17T04:25:37.491288Z",
     "iopub.status.idle": "2025-09-17T07:40:34.532618Z",
     "shell.execute_reply": "2025-09-17T07:40:34.531706Z",
     "shell.execute_reply.started": "2025-09-17T04:25:37.491475Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58/58 3:11:30, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.518800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=58, training_loss=2.605840510335462, metrics={'train_runtime': 11694.7493, 'train_samples_per_second': 0.645, 'train_steps_per_second': 0.005, 'total_flos': 7.283218325354496e+16, 'train_loss': 2.605840510335462, 'epoch': 1.9661016949152543})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "sft_cfg = SFTConfig(\n",
    "    output_dir=\"qlora_cls\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=20,\n",
    "    save_steps=500,\n",
    "    bf16=(DTYPE==torch.bfloat16),\n",
    "    fp16=(DTYPE==torch.float16),\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_seq_length=384,\n",
    "    report_to=[],\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_cfg,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,   # use custom collator that pads labels with -100\n",
    "    packing=False,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T07:40:34.533750Z",
     "iopub.status.busy": "2025-09-17T07:40:34.533504Z",
     "iopub.status.idle": "2025-09-17T07:40:34.546938Z",
     "shell.execute_reply": "2025-09-17T07:40:34.546050Z",
     "shell.execute_reply.started": "2025-09-17T07:40:34.533728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Constrained decoding for evaluation --------------------\n",
    "class LabelOnlyProcessor(LogitsProcessor):\n",
    "    def __init__(self, allowed_ids, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.allowed = torch.tensor(allowed_ids, device=device)\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Mask everything except class tokens\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        mask.index_fill_(1, self.allowed, 0.0)\n",
    "        return scores + mask\n",
    "\n",
    "allowed_ids = tokenizer.convert_tokens_to_ids(list(label_to_token.values()))\n",
    "processor = LabelOnlyProcessor(allowed_ids)\n",
    "\n",
    "def predict_labels(texts, model, batch_size=32, max_input_tokens=512):\n",
    "    preds = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        prompts = [build_chat_prompt(t) for t in batch]\n",
    "        enc = tokenizer(prompts, return_tensors=\"pt\",\n",
    "                        padding=True, truncation=True, max_length=max_input_tokens).to(model.device)\n",
    "        with torch.inference_mode():\n",
    "            gen = model.generate(**enc,\n",
    "                                 max_new_tokens=1,           # we want exactly one label token\n",
    "                                 do_sample=False,            # greedy\n",
    "                                 logits_processor=[processor],\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.pad_token_id,\n",
    "                                 use_cache=True)\n",
    "        # take only new tokens after the prompt\n",
    "        new = gen[:, enc[\"input_ids\"].shape[1]:]\n",
    "        toks = tokenizer.batch_decode(new, skip_special_tokens=False)\n",
    "        # map token -> class (first token)\n",
    "        for t in toks:\n",
    "            tok = t.strip().split()[0] if t.strip() else \"\"\n",
    "            preds.append(token_to_label.get(tok, \"нет товара\"))\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T07:40:34.547808Z",
     "iopub.status.busy": "2025-09-17T07:40:34.547554Z",
     "iopub.status.idle": "2025-09-17T07:45:58.919250Z",
     "shell.execute_reply": "2025-09-17T07:45:58.918502Z",
     "shell.execute_reply.started": "2025-09-17T07:40:34.547784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `70` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                 обувь     0.6250    0.5556    0.5882        27\n",
      "                одежда     0.9404    0.7571    0.8388       354\n",
      "                посуда     1.0000    1.0000    1.0000        16\n",
      "              текстиль     1.0000    0.4783    0.6471        23\n",
      "      товары для детей     1.0000    0.6250    0.7692        16\n",
      "украшения и аксессуары     0.9167    0.5500    0.6875        20\n",
      "           электроника     1.0000    0.8000    0.8889        15\n",
      "            нет товара     0.6250    0.9487    0.7536       195\n",
      "\n",
      "              accuracy                         0.7928       666\n",
      "             macro avg     0.8884    0.7143    0.7717       666\n",
      "          weighted avg     0.8408    0.7928    0.7959       666\n",
      "\n",
      "Weighted F1: 0.7958554186829252\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Weighted F1 on validation --------------------\n",
    "y_true = val_df[\"label\"].tolist()\n",
    "y_pred = predict_labels(val_df[\"text\"].tolist(), batch_size=32)\n",
    "\n",
    "print(classification_report(y_true, y_pred, labels=CLASSES, digits=4))\n",
    "print(\"Weighted F1:\", f1_score(y_true, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T07:45:58.920299Z",
     "iopub.status.busy": "2025-09-17T07:45:58.920036Z",
     "iopub.status.idle": "2025-09-17T07:46:13.177800Z",
     "shell.execute_reply": "2025-09-17T07:46:13.177033Z",
     "shell.execute_reply.started": "2025-09-17T07:45:58.920269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "OUT_DIR = \"qlora_cls_adapter\"\n",
    "\n",
    "# 1) Save the PEFT adapter weights/config (small)\n",
    "model.save_pretrained(OUT_DIR)          # or: trainer.save_model(OUT_DIR)\n",
    "\n",
    "# 2) Save tokenizer (so special class tokens are preserved)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "# 3) Save your label↔token mapping for inference\n",
    "with open(os.path.join(OUT_DIR, \"label_tokens.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"label_to_token\": label_to_token}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 4) (Optional) Save training state/args for reproducibility\n",
    "trainer.save_state()\n",
    "with open(os.path.join(OUT_DIR, \"training_args.json\"), \"w\") as f:\n",
    "    f.write(trainer.args.to_json_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T07:58:02.375540Z",
     "iopub.status.busy": "2025-09-17T07:58:02.374914Z",
     "iopub.status.idle": "2025-09-17T08:10:41.890592Z",
     "shell.execute_reply": "2025-09-17T08:10:41.889807Z",
     "shell.execute_reply.started": "2025-09-17T07:58:02.375498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/qlora_cls/ (stored 0%)\n",
      "  adding: kaggle/working/qlora_cls/trainer_state.json (deflated 59%)\n",
      "  adding: kaggle/working/qlora_cls/checkpoint-58/ (stored 0%)\n",
      "  adding: kaggle/working/qlora_cls/checkpoint-58/tokenizer_config.json (deflated 89%)\n",
      "  adding: kaggle/working/qlora_cls/checkpoint-58/vocab.json (deflated 69%)\n",
      "  adding: kaggle/working/qlora_cls/checkpoint-58/adapter_model.safetensors^C\n",
      "\n",
      "\n",
      "\n",
      "zip error: Interrupted (aborting)\n",
      "  adding: kaggle/working/qlora_cls_adapter/ (stored 0%)\n",
      "  adding: kaggle/working/qlora_cls_adapter/tokenizer_config.json (deflated 89%)\n",
      "  adding: kaggle/working/qlora_cls_adapter/vocab.json (deflated 69%)\n",
      "  adding: kaggle/working/qlora_cls_adapter/adapter_model.safetensors^C\n",
      "\n",
      "\n",
      "\n",
      "zip error: Interrupted (aborting)\n"
     ]
    }
   ],
   "source": [
    "!zip -r qlora_cls.zip /kaggle/working/qlora_cls\n",
    "!zip -r qlora_cls_adapter /kaggle/working/qlora_cls_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T07:55:16.631571Z",
     "iopub.status.busy": "2025-09-17T07:55:16.630927Z",
     "iopub.status.idle": "2025-09-17T07:55:16.634915Z",
     "shell.execute_reply": "2025-09-17T07:55:16.634289Z",
     "shell.execute_reply.started": "2025-09-17T07:55:16.631548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_qlora = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json, torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "DTYPE = torch.float16 if torch.cuda.get_device_capability()[0] < 8 else torch.bfloat16\n",
    "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=DTYPE,\n",
    "                             bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "ADAPTER_DIR = \"qlora_cls_adapter\"\n",
    "\n",
    "# AutoPeftModel will read base_model_name_or_path from the adapter config and load base+adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    ADAPTER_DIR, quantization_config=bnb_cfg, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=False)\n",
    "with open(f\"{ADAPTER_DIR}/label_tokens.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label_to_token = json.load(f)[\"label_to_token\"]\n",
    "token_to_label = {v:k for k,v in label_to_token.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разметка тестового набора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T08:10:58.246150Z",
     "iopub.status.busy": "2025-09-17T08:10:58.245866Z",
     "iopub.status.idle": "2025-09-17T08:10:58.260816Z",
     "shell.execute_reply": "2025-09-17T08:10:58.260114Z",
     "shell.execute_reply.started": "2025-09-17T08:10:58.246119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "DTYPE = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=DTYPE, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "for param in base.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T08:40:56.319739Z",
     "iopub.status.busy": "2025-09-17T08:40:56.319026Z",
     "iopub.status.idle": "2025-09-17T08:40:56.655208Z",
     "shell.execute_reply": "2025-09-17T08:40:56.654398Z",
     "shell.execute_reply.started": "2025-09-17T08:40:56.319717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"t-tech/T-lite-it-1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T08:11:01.162499Z",
     "iopub.status.busy": "2025-09-17T08:11:01.162249Z",
     "iopub.status.idle": "2025-09-17T08:11:01.166985Z",
     "shell.execute_reply": "2025-09-17T08:11:01.166364Z",
     "shell.execute_reply.started": "2025-09-17T08:11:01.162482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classificator_prompt = '''\n",
    "System: Ты — аккуратный классификатор отзывов маркетплейса по категориям товаров, к которым они относятся.\n",
    "Доступные метки:\n",
    "обувь,\n",
    "одежда,\n",
    "посуда,\n",
    "текстиль,\n",
    "товары для детей,\n",
    "украшения и аксессуары,\n",
    "электроника,\n",
    "нет товара.\n",
    "\n",
    "Правила:\n",
    "- Если отзыв только про доставку/продавца/деньги/спор без товара → \"нет товара\".\n",
    "- Если не ясно, какой товар — тоже \"нет товара\".\n",
    "- Выбери ровно одну метку.\n",
    "\n",
    "Ответь строго JSON одной строкой:\n",
    "{\"label\": \"<одна метка>\", \"confidence\": 0..1, \"reason\": \"<до 12 слов>\"}\n",
    "\n",
    "Примеры:\n",
    "\n",
    "Отзыв: отличная блузка отслеживалась пришла за 10 дней.\n",
    "Ответ: {\"label\": \"одежда\", \"confidence\": 1, \"reason\": \"блузка относится к одежде\"}\n",
    "\n",
    "Отзыв: Лучшая покупка на алиэкспресс !!)))\n",
    "Ответ: {\"label\": \"нет товара\", \"confidence\": 1, \"reason\": \"нет никакого описания товара\"}\n",
    "\n",
    "Отзыв: На фото они матовые, а пришли глянцевые.\n",
    "Ответ: {\"label\": \"обувь\", \"confidence\": 0.4, \"reason\": \"матовость и глянцевость в множественном числе, вероятно, относится к обуви\"}\n",
    "\n",
    "Отзыв: Товар пришёл менее чем за месяц. На мой 44 заказала L пришла необъятных размеров  \n",
    "Ответ: {\"label\": \"одежда\", \"confidence\": 0.9, \"reason\": \"Размер L почти всегда относится к одежде\"}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T08:41:02.327791Z",
     "iopub.status.busy": "2025-09-17T08:41:02.327045Z",
     "iopub.status.idle": "2025-09-17T08:46:34.763812Z",
     "shell.execute_reply": "2025-09-17T08:46:34.762885Z",
     "shell.execute_reply.started": "2025-09-17T08:41:02.327765Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b311daa7db4f31b25e01f9a6821dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM labeling:   0%|          | 0/910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `70` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/2922909992.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# per-sample decode (robust for Qwen2 slow tokenizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mout_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msafe_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_texts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/2922909992.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# per-sample decode (robust for Qwen2 slow tokenizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mout_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msafe_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_texts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/2922909992.py\u001b[0m in \u001b[0;36msafe_decode\u001b[0;34m(ids_tensor)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/tokenization_qwen2.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;31m# `spaces_between_special_tokens` defaults to True for _decode in slow tokenizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# and cannot be configured elsewhere, but it should default to False for Qwen2Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         return super().decode(\n\u001b[0m\u001b[1;32m    300\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   4014\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4016\u001b[0;31m         return self._decode(\n\u001b[0m\u001b[1;32m   4017\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4018\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcurrent_sub_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_sub_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m             \u001b[0msub_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_sub_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspaces_between_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/tokenization_qwen2.py\u001b[0m in \u001b[0;36mconvert_tokens_to_string\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_tokens_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;34m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_decoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "import re, json, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- tokenizer hygiene ---\n",
    "# make sure both pad_token_id and pad_token are set (Qwen2 slow tokenizer can be picky)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "base.eval()\n",
    "\n",
    "# ---- helpers (as before) ----\n",
    "def make_prompt(txt: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": classificator_prompt},\n",
    "        {\"role\": \"user\", \"content\": txt},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def extract_json(s: str):\n",
    "    try:\n",
    "        i, j = s.find(\"{\"), s.rfind(\"}\")\n",
    "        if i != -1 and j != -1 and j > i:\n",
    "            return json.loads(s[i:j+1])\n",
    "    except Exception:\n",
    "        pass\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def parse_output(text: str):\n",
    "    obj = extract_json(text) or {}\n",
    "    label = obj.get(\"label\", None)\n",
    "    conf  = obj.get(\"confidence\", None)\n",
    "    if isinstance(label, str):\n",
    "        label = label.strip().lower()\n",
    "    if label not in CLASSES:\n",
    "        for c in CLASSES:\n",
    "            if label and c in label:\n",
    "                label = c; break\n",
    "    if label not in CLASSES:\n",
    "        label = \"нет товара\"\n",
    "    try:\n",
    "        conf = float(conf)\n",
    "        if not (0.0 <= conf <= 1.0):\n",
    "            conf = 0.5\n",
    "    except Exception:\n",
    "        conf = 0.5\n",
    "    return label, conf\n",
    "\n",
    "# --- data ---\n",
    "df_test[\"text\"] = df_test[\"text\"].fillna(\"\").astype(str)\n",
    "texts = df_test[\"text\"].tolist()\n",
    "\n",
    "# --- generation settings (clean: no sampling-only args) ---\n",
    "BATCH_SIZE = 8\n",
    "MAX_INPUT_TOKENS = 512\n",
    "MAX_NEW_TOKENS = 24   # JSON is short; keep small for speed\n",
    "\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    do_sample=False,                      # greedy\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "pred_labels, pred_conf = [], []\n",
    "\n",
    "# safe per-row decode to avoid None tokens from padded matrix\n",
    "def safe_decode(ids_tensor):\n",
    "    # filter out any PAD / negative ids before decode\n",
    "    ids = ids_tensor.tolist()\n",
    "    ids = [i for i in ids if isinstance(i, int) and i >= 0 and i != tokenizer.pad_token_id]\n",
    "    if not ids:\n",
    "        return \"\"\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for start in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"LLM labeling\"):\n",
    "        batch_texts = texts[start:start+BATCH_SIZE]\n",
    "        prompts = [make_prompt(t) for t in batch_texts]\n",
    "\n",
    "        enc = tokenizer(\n",
    "            prompts, return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=MAX_INPUT_TOKENS\n",
    "        ).to(base.device)\n",
    "\n",
    "        input_lengths = enc[\"attention_mask\"].sum(dim=1)\n",
    "\n",
    "        gen = base.generate(**enc, **GEN_KW)\n",
    "\n",
    "        # slice only the newly generated tokens\n",
    "        new_tokens = [out_ids[in_len:] for out_ids, in_len in zip(gen, input_lengths)]\n",
    "\n",
    "        # per-sample decode (robust for Qwen2 slow tokenizer)\n",
    "        out_texts = [safe_decode(t) for t in new_tokens]\n",
    "\n",
    "        for out in out_texts:\n",
    "            lbl, conf = parse_output(out)\n",
    "            pred_labels.append(lbl)\n",
    "            pred_conf.append(conf)\n",
    "\n",
    "df_test[\"class_from_llm\"] = pred_labels\n",
    "df_test[\"llm_confidence\"] = pred_conf\n",
    "\n",
    "print(df_test[\"class_from_llm\"].value_counts(dropna=False))\n",
    "print(f\"Mean confidence: {np.mean(pred_conf):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test.to_parquet(\"test_markup_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "L_test = applier.apply(df=df_test)  # shape (N, num_LFs)\n",
    "test_probs = label_model.predict_proba(L_test)  # (N, 8)\n",
    "lf_pred = test_probs.argmax(1)\n",
    "lf_conf = test_probs.max(1)\n",
    "\n",
    "df_test['prediction_snorkel'] = lf_pred\n",
    "df_test['snorkel_confidence'] = lf_conf\n",
    "df_test[\"snorkel_probs\"] = test_probs.tolist()\n",
    "\n",
    "df_test.to_parquet('test_markup_snorkel.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "p_snorkel = df_test['snorkel_probs'].values\n",
    "p_snorkel = np.array(list(p_snorkel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "p_llm = build_llm_probs_from_label(df_test[\"class_from_llm\"], df_test[\"llm_confidence\"], K=len(CLASSES))\n",
    "y, conf, p = fuse_probs(p_snorkel, p_llm, llm_conf=df_test[\"llm_confidence\"], tau_low=0.50)\n",
    "final_labels = [CLASSES[i] for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test['label'] = final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------- Weighted F1 on validation --------------------\n",
    "y_true = df_test[\"label\"].tolist()\n",
    "y_pred = predict_labels(df_test[\"text\"].tolist(), model=model_qlora, batch_size=32)\n",
    "\n",
    "print(classification_report(y_true, y_pred, labels=CLASSES, digits=4))\n",
    "print(\"Weighted F1:\", f1_score(y_true, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=DTYPE, device_map=\"auto\")\n",
    "base.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "pt_cfg = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_virtual_tokens=64,  # 20-100; more = more capacity\n",
    "    tokenizer_name_or_path=BASE_MODEL,\n",
    ")\n",
    "model = get_peft_model(base, pt_cfg)\n",
    "\n",
    "sft_cfg = SFTConfig(\n",
    "    output_dir=\"prompt_tune_cls\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4, gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-4, warmup_ratio=0.05,\n",
    "    logging_steps=20, save_steps=500, bf16=(DTYPE==torch.bfloat16),\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "trainer = SFTTrainer(model=model, train_dataset=ds[\"train\"], eval_dataset=ds[\"validation\"],\n",
    "                     tokenizer=tokenizer, args=sft_cfg)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SMALL_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # example\n",
    "tokenizer = AutoTokenizer.from_pretrained(SMALL_MODEL, use_fast=True)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": specials})\n",
    "model = AutoModelForCausalLM.from_pretrained(SMALL_MODEL, torch_dtype=DTYPE, device_map=\"auto\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "sft_cfg = SFTConfig(\n",
    "    output_dir=\"fullft_cls_small\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4, gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5, weight_decay=0.01, warmup_ratio=0.03,\n",
    "    bf16=(DTYPE==torch.bfloat16), logging_steps=20, save_steps=500\n",
    ")\n",
    "trainer = SFTTrainer(model=model, train_dataset=ds[\"train\"], eval_dataset=ds[\"validation\"],\n",
    "                     tokenizer=tokenizer, args=sft_cfg)\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8260993,
     "sourceId": 13045843,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8269471,
     "sourceId": 13075715,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
